[{"content":"安装hugo Hugo官网： The world’s fastest framework for building websites\n下载下面这个即可：\n将压缩包解压到 bin 目录下：\n配置环境变量 配置环境变量，就可以在任意位置使用hugo命令了\n任意位置打开cmd，输入hugo version验证一下\n搭建博客 创建博客 在任意位置打开cmd，输入：hugo new site xxx\n可以看到对应位置出现文件夹\ncd进入对应的目录，然后使用命令启动hugo\n1 2 3 4 5 hugo server --buildDrafts # 或者简写 hugo server -D # 或者 hugo server 在浏览器输入 http://localhost:1313，出现Page Not Found则表示成功，Page Not Found是因为还没配主题\n配置主题 在hugo官网找一个喜欢的主题下载即可\n选择主题的源码并下载\n解压并放到themes目录下\n打开exampleSite文件夹，将 Content 和 hugo.yaml 复制到主文件夹下\n修改theme为themes文件夹下的主题名，以后需要换一个主题也是修改这里\n重新启动hugo即可，相关配置修改**hugo.yaml**\nstack官网：\nStack | Card-style Hugo theme designed for bloggers (jimmycai.com)\n写博客 在指定位置打开cmd 创建index.md 1 hugo new content post/MyFirstBlog/index.md 其中/MyFirstBlog可以任意命名\n/index.md 不能任意命名，否则图片等资源无法显示\n最后，就可以愉快的在index.md中写博客了\n","date":"2024-09-25T21:27:40+08:00","image":"https://gcore.jsdelivr.net/gh/XXL999227/image/img/20241010210254.png","permalink":"https://xxl999227.github.io/archives/hugo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/","title":"HUGO搭建博客"},{"content":"初识elasticsearch 什么是elasticsearch\nelasticsearch是一款非常强大的开源搜索引擎，可以帮助我们从海量数据中快速找到需要的内容。\nelasticsearch结合kibana、Logstash、Beats，也就是elastic stack（ELK）（是以elasticsearch为核心的技术栈，包括beats、Logstash、kibana、elasticsearch）。被广泛应用在日志数据分析、实时监控等领域。\nelasticsearch是elastic stack的核心，负责存储、搜索、分析数据。\nelasticsearch的发展\nLucene是一个Java语言的搜索引擎类库，是Apache公司的顶级项目，由DougCutting于1999年研发。官网地址：https://lucene.apache.org/。\nLucene的优势：\n:::danger 易扩展\n高性能（基于倒排索引）\n:::\nLucene的缺点：\n:::color4 只限于Java语言开发\n学习曲线陡峭\n不支持水平扩展\n:::\n2004年Shay Banon基于Lucene开发了Compass\n2010年Shay Banon重写了Compass，取名为Elasticsearch。\n官网地址: https://www.elastic.co/cn/\n相比与lucene，elasticsearch具备下列优势：\n:::success 支持分布式，可水平扩展\n提供Restful接口，可被任何语言调用\n:::\n正向索引和倒排索引 传统数据库（如MySQL）采用正向索引，例如给下表（tb_goods）中的id创建索引：\nelasticsearch采用倒排索引：\n:::success 文档（document）：每条数据就是一个文档\n词条（term）：文档按照语义分成的词语\n:::\n总结：\n什么是文档和词条？\n:::success 每一条数据就是一个文档\n对文档中的内容分词，得到的词语就是词条\n:::\n什么是正向索引？\n:::warning 基于文档id创建索引。查询词条时必须先找到文档，而后判断是否包含词条\n:::\n什么是倒排索引？\n:::color1 对文档内容分词，对词条创建索引，并记录词条所在文档的信息。查询时先根据词条查询到文档id，而后获取到文档\n:::\nelasticsearch是面向文档存储的，可以是数据库中的一条商品数据，一个订单信息。\n文档数据会被序列化为json格式后存储在elasticsearch中。\nmysql和elasticsearch概念对比 MySQL Elasticsearch 说明 Table Index 索引(index)，就是文档的集合，类似数据库的表(table) Row Document 文档（Document），就是一条条的数据，类似数据库中的行（Row），文档都是JSON格式 Column Field 字段（Field），就是JSON文档中的字段，类似数据库中的列（Column） Schema Mapping Mapping（映射）是索引中文档的约束，例如字段类型约束。类似数据库的表结构（Schema） SQL DSL DSL是elasticsearch提供的JSON风格的请求语句，用来操作elasticsearch，实现CRUD Mysql：擅长事务类型操作，可以确保数据的安全和一致性\nElasticsearch：擅长海量数据的搜索、分析、计算\n总结：\n文档：一条数据就是一个文档，es中是Json格式\n字段：Json文档中的字段\n索引：同类型文档的集合\n映射：索引中文档的约束，比如字段名称、类型\nelasticsearch与数据库的关系：\n数据库负责事务类型操作\nelasticsearch负责海量数据的搜索、分析、计算\n3、安装elasticsearch、kibana、IK分词器 采用docker安装它们\n部署单点es 创建网络\n因为我们还需要部署kibana容器，因此需要让es和kibana容器互联。这里先创建一个网络：\n1 docker network create es-net 运行docker命令，部署单点es：\n1 2 3 4 5 6 7 8 9 10 11 docker run -d \\ --name es \\ -e \u0026#34;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026#34; \\ -e \u0026#34;discovery.type=single-node\u0026#34; \\ -v es-data:/usr/share/elasticsearch/data \\ -v es-plugins:/usr/share/elasticsearch/plugins \\ --privileged \\ --network es-net \\ -p 9200:9200 \\ -p 9300:9300 \\ elasticsearch:7.12.1 命令解释：\n-e \u0026quot;cluster.name=es-docker-cluster\u0026quot;：设置集群名称 -e \u0026quot;http.host=0.0.0.0\u0026quot;：监听的地址，可以外网访问 -e \u0026quot;ES_JAVA_OPTS=-Xms512m -Xmx512m\u0026quot;：内存大小 -e \u0026quot;discovery.type=single-node\u0026quot;：非集群模式 -v es-data:/usr/share/elasticsearch/data：挂载逻辑卷，绑定es的数据目录 -v es-logs:/usr/share/elasticsearch/logs：挂载逻辑卷，绑定es的日志目录 -v es-plugins:/usr/share/elasticsearch/plugins：挂载逻辑卷，绑定es的插件目录 --privileged：授予逻辑卷访问权 --network es-net ：加入一个名为es-net的网络中 -p 9200:9200：端口映射配置 在浏览器中输入：http://虚拟机ip:9200 即可看到elasticsearch的响应结果：\n部署kibana kibana可以给我们提供一个elasticsearch的可视化界面，便于我们学习。\n运行docker命令，部署kibana：\n1 2 3 4 5 6 docker run -d \\ --name kibana \\ -e ELASTICSEARCH_HOSTS=http://es:9200 \\ --network=es-net \\ -p 5601:5601 \\ kibana:7.12.1 kibana启动一般比较慢，需要多等待一会，可以通过命令：\n1 docker logs -f kibana |grep 5601 查看运行日志，当查看到下面的日志，说明成功：\n此时，在浏览器输入地址访问：http://虚拟机ip:5601，即可看到结果\n安装IK分词器 在线安装：\n1 2 3 4 5 6 7 8 9 10 11 # 进入容器内部 docker exec -it es /bin/bash # 在线下载并安装 ./bin/elasticsearch-plugin install https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.12.1/elasticsearch-analysis-ik-7.12.1.zip #退出 exit #重启容器 docker restart es IK分词器包含两种模式：\nik_smart：最少切分，粒度较粗\nik_max_word：最细切分，粒度细\n进入es的Dev Tools\n扩展词典和停用词典 随着互联网的发展，“造词运动”也越发的频繁。出现了很多新的词语，在原有的词汇列表中并不存在。比如：“奥力给”，“传智播客” 等。\n所以我们的词汇也需要不断的更新，IK分词器提供了扩展词汇的功能。\n1）打开IK分词器config目录：\n2）在IKAnalyzer.cfg.xml配置文件内容添加：\n1 2 3 4 5 6 7 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE properties SYSTEM \u0026#34;http://java.sun.com/dtd/properties.dtd\u0026#34;\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;comment\u0026gt;IK Analyzer 扩展配置\u0026lt;/comment\u0026gt; \u0026lt;!--用户可以在这里配置自己的扩展字典 *** 添加扩展词典--\u0026gt; \u0026lt;entry key=\u0026#34;ext_dict\u0026#34;\u0026gt;ext.dic\u0026lt;/entry\u0026gt; \u0026lt;/properties\u0026gt; 3）新建一个 ext.dic，可以参考config目录下复制一个配置文件进行修改\n1 2 传智播客 奥力给 4）重启elasticsearch\n1 2 3 4 docker restart es # 查看 日志 docker logs -f elasticsearch 日志中已经成功加载ext.dic配置文件\n停用词典：\n在互联网项目中，在网络间传输的速度很快，所以很多语言是不允许在网络上传递的，如：关于宗教、政治等敏感词语，那么我们在搜索时也应该忽略当前词汇。\nIK分词器也提供了强大的停用词功能，让我们在索引时就直接忽略当前的停用词汇表中的内容。\n1）IKAnalyzer.cfg.xml配置文件内容添加：\n1 2 3 4 5 6 7 8 9 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;!DOCTYPE properties SYSTEM \u0026#34;http://java.sun.com/dtd/properties.dtd\u0026#34;\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;comment\u0026gt;IK Analyzer 扩展配置\u0026lt;/comment\u0026gt; \u0026lt;!--用户可以在这里配置自己的扩展字典--\u0026gt; \u0026lt;entry key=\u0026#34;ext_dict\u0026#34;\u0026gt;ext.dic\u0026lt;/entry\u0026gt; \u0026lt;!--用户可以在这里配置自己的扩展停止词字典 *** 添加停用词词典--\u0026gt; \u0026lt;entry key=\u0026#34;ext_stopwords\u0026#34;\u0026gt;stopword.dic\u0026lt;/entry\u0026gt; \u0026lt;/properties\u0026gt; 3）在 stopword.dic 添加停用词\n1 习大大 4）重启elasticsearch\n1 2 3 4 5 6 # 重启服务 docker restart es docker restart kibana # 查看 日志 docker logs -f es 日志中已经成功加载stopword.dic配置文件\n注意当前文件的编码必须是 UTF-8 格式，严禁使用Windows记事本编辑\n总结：\n分词器的作用是什么？\n:::warning 创建倒排索引时对文档分词\n用户搜索时，对输入的内容分词\n:::\nIK分词器有几种模式？\n:::color1 ik_smart：智能切分，粗粒度\nik_max_word：最细切分，细粒度\n:::\nIK分词器如何拓展词条？如何停用词条？\n:::danger 利用config目录的IkAnalyzer.cfg.xml文件添加拓展词典和停用词典\n在词典中添加拓展词条或者停用词条\n:::\n数据同步 elasticsearch中的酒店数据来自于mysql数据库，因此mysql数据发生改变时，elasticsearch也必须跟着改变，这个就是elasticsearch与mysql之间的数据同步。\n总结：\n方式一：同步调用\n:::info 优点：实现简单，粗暴\n缺点：业务耦合度高\n:::\n方式二：异步通知\n:::color2 优点：低耦合，实现难度一般\n缺点：依赖mq的可靠性\n:::\n方式三：监听binlog\n:::color3 优点：完全解除服务间耦合\n缺点：开启binlog增加数据库负担、实现复杂度高\n:::\n","date":"2024-05-09T21:58:16+08:00","image":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A111elasticsearch/img/image-17.png","permalink":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A111elasticsearch/","title":"【微服务】11、ElasticSearch"},{"content":"初识MQ 同步调用的问题 同步调用的优点：\n:::color1 •时效性较强，可以立即得到结果\n:::\n同步调用的问题：\n:::danger •耦合度高\n•性能和吞吐能力下降\n•有额外的资源消耗\n•有级联失败问题\n:::\n异步调用 异步通信的优点：\n:::danger •耦合度低\n•吞吐量提升\n•故障隔离\n•流量削峰\n:::\n异步通信的缺点：\n:::success •依赖于Broker的可靠性、安全性、吞吐能力\n•架构复杂了，业务没有明显的流程线，不好追踪管理\n:::\n不同MQ对比 追求可用性：Kafka、 RocketMQ 、RabbitMQ\n追求可靠性：RabbitMQ、RocketMQ\n追求吞吐能力：RocketMQ、Kafka\n追求消息低延迟：RabbitMQ、Kafka\nRabbitMQ快速入门 RabbitMQ是基于Erlang语言开发的开源消息通信中间件，官网地址：https://www.rabbitmq.com/\n安装RabbitMQ 我们在Centos7虚拟机中使用Docker来安装。\n这里选用在线拉取镜像的方式\n1 docker pull rabbitmq:3-management 执行下面的命令来运行MQ容器：\n1 2 3 4 5 6 7 8 9 docker run \\ -e RABBITMQ_DEFAULT_USER=itcast \\ -e RABBITMQ_DEFAULT_PASS=123321 \\ --name mq \\ --hostname mq1 \\ -p 15672:15672 \\ -p 5672:5672 \\ -d \\ rabbitmq:3-management 安装完成后即可登录：\nRabbitMQ的结构和概念 RabbitMQ中的几个概念：\n:::success\nchannel：通道，操作MQ的工具 exchange：交换机，路由消息到队列中 queue：队列，存放消息 virtual host：虚拟主机，是对queue、exchange等资源的逻辑分组，可用它支持多租户模式，可以理解成namespace :::\n常见消息模型 MQ的官方文档RabbitMQ Tutorials | RabbitMQ中给出了5个MQ的Demo示例，对应了几种不同的用法：\n简单队列模型（BasicQueue） 官方的HelloWorld是基于最基础的消息队列模型来实现的，只包括三个角色：\n:::color1\npublisher：消息发布者，将消息发送到队列queue queue：消息队列，负责接受并缓存消息 consumer：订阅队列，处理队列中的消息 :::\n案例：\nPublisherTest：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 package cn.itcast.mq.helloworld; import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import org.junit.Test; import java.io.IOException; import java.util.concurrent.TimeoutException; public class PublisherTest { @Test public void testSendMessage() throws IOException, TimeoutException { // 1.建立连接 ConnectionFactory factory = new ConnectionFactory(); // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码 factory.setHost(\u0026#34;192.168.79.128\u0026#34;); factory.setPort(5672); factory.setVirtualHost(\u0026#34;/\u0026#34;); factory.setUsername(\u0026#34;itcast\u0026#34;); factory.setPassword(\u0026#34;123321\u0026#34;); // 1.2.建立连接 Connection connection = factory.newConnection(); // 2.创建通道Channel Channel channel = connection.createChannel(); // 3.创建队列 String queueName = \u0026#34;simple.queue\u0026#34;; channel.queueDeclare(queueName, false, false, false, null); // 4.发送消息 String message = \u0026#34;hello, rabbitmq!\u0026#34;; channel.basicPublish(\u0026#34;\u0026#34;, queueName, null, message.getBytes()); System.out.println(\u0026#34;发送消息成功：【\u0026#34; + message + \u0026#34;】\u0026#34;); // 5.关闭通道和连接 channel.close(); connection.close(); } } 信息发送完成后：\nConsumerTest：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 package cn.itcast.mq.helloworld; import com.rabbitmq.client.*; import java.io.IOException; import java.util.concurrent.TimeoutException; public class ConsumerTest { public static void main(String[] args) throws IOException, TimeoutException { // 1.建立连接 ConnectionFactory factory = new ConnectionFactory(); // 1.1.设置连接参数，分别是：主机名、端口号、vhost、用户名、密码 factory.setHost(\u0026#34;192.168.79.128\u0026#34;); factory.setPort(5672); factory.setVirtualHost(\u0026#34;/\u0026#34;); factory.setUsername(\u0026#34;itcast\u0026#34;); factory.setPassword(\u0026#34;123321\u0026#34;); // 1.2.建立连接 Connection connection = factory.newConnection(); // 2.创建通道Channel Channel channel = connection.createChannel(); // 3.创建队列 String queueName = \u0026#34;simple.queue\u0026#34;; channel.queueDeclare(queueName, false, false, false, null); // 4.订阅消息 channel.basicConsume(queueName, true, new DefaultConsumer(channel){ @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { // 5.处理消息 String message = new String(body); System.out.println(\u0026#34;接收到消息：【\u0026#34; + message + \u0026#34;】\u0026#34;); } }); System.out.println(\u0026#34;等待接收消息。。。。\u0026#34;); } } 控制台：\n基本消息队列的消息发送流程：\n:::color2\n建立connection 创建channel 利用channel声明队列 利用channel向队列发送消息 :::\n基本消息队列的消息接收流程：\n:::color2\n建立connection 创建channel 利用channel声明队列 定义consumer的消费行为handleDelivery() 利用channel将消费者与队列绑定 :::\nSpringAMQP实现简单队列 什么是SpringAMQP\nSpringAmqp的官方地址：Spring AMQP\n利用SpringAMQP实现HelloWorld中的基础消息队列功能：\n流程如下：\n1.在父工程中引入spring-amqp的依赖\n2.在publisher服务中利用RabbitTemplate发送消息到simple.queue这个队列\n3.在consumer服务中编写消费逻辑，绑定simple.queue这个队列\n步骤1：引入AMQP依赖\n因为publisher和consumer服务都需要amqp依赖，因此这里把依赖直接放到父工程mq-demo中：\n1 2 3 4 5 \u0026lt;!--AMQP依赖，包含RabbitMQ--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-boot-starter-amqp\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 步骤2：在publisher中编写测试方法，向simple.queue发送消息\n1.在publisher服务中编写application.yml，添加mq连接信息：\n1 2 3 4 5 6 7 spring: rabbitmq: host: 192.168.79.128 port: 5672 virtual-host: / username: itcast password: 123321 2.在publisher服务中新建一个测试类，编写测试方法：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 @RunWith(SpringRunner.class) @SpringBootTest public class SpringAmqpTest { @Autowired private RabbitTemplate rabbitTemplate; @Test public void testSendMessage() { String message = \u0026#34;hello, spring amqp!\u0026#34;; rabbitTemplate.convertAndSend(\u0026#34;simple.queue\u0026#34;, message); System.out.println(\u0026#34;发送消息成功：【\u0026#34; + message + \u0026#34;】\u0026#34;); } } 步骤3：在consumer中编写消费逻辑，监听simple.queue\n1.在consumer服务中编写application.yml，添加mq连接信息：\n1 2 3 4 5 6 7 spring: rabbitmq: host: 192.168.79.128 port: 5672 virtual-host: / username: itcast password: 123321 2.在consumer服务中新建一个类，编写消费逻辑：\n1 2 3 4 5 6 7 @Component public class SpringRabbitListener { @RabbitListener(queues = \u0026#34;simple.queue\u0026#34;) public void listen(String message) { System.out.println(\u0026#34;接收到simple.queue消息：\u0026#34; + message); } } Work Queue 工作队列 步骤1：生产者循环发送消息到simple.queue\n在publisher服务中添加一个测试方法，循环发送50条消息到simple.queue队列\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @Resource private RabbitAdmin rabbitAdmin; /** * 测试发送消息到工作队列 */ @Test public void testSendMessageToWorkQueue() { // 创建队列 Queue queue = new Queue(\u0026#34;work.queue\u0026#34;, true, false, false); rabbitAdmin.declareQueue(queue); for (int i = 1; i \u0026lt;= 50; i++) { String message = \u0026#34;hello, spring amqp!————\u0026#34; + i; rabbitTemplate.convertAndSend(\u0026#34;work.queue\u0026#34;, message); System.out.println(\u0026#34;发送消息成功：【\u0026#34; + message + \u0026#34;】\u0026#34;); } } 步骤2：编写两个消费者，都监听simple.queue\n在consumer服务中添加一个消费者，也监听simple.queue：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 /** * 监听工作队列 * * @param message 消息 */ @RabbitListener(queues = \u0026#34;work.queue\u0026#34;) public void listenWorkQueue1(String message) throws InterruptedException { System.out.println(\u0026#34;消费者1接收到work.queue消息：\u0026#34; + message + \u0026#34; time：\u0026#34; + LocalTime.now()); Thread.sleep(20); } @RabbitListener(queues = \u0026#34;work.queue\u0026#34;) public void listenWorkQueue2(String message) throws InterruptedException { System.err.println(\u0026#34;消费者2。。。接收到work.queue消息：\u0026#34; + message + \u0026#34; time：\u0026#34; + LocalTime.now()); Thread.sleep(200); } 消息大概消费了五秒钟：\n这是因为有消费预取的原因\n消费预取限制\n修改application.yml文件，设置preFetch这个值（默认是无限制），可以控制预取消息的上限：\n1 2 3 4 5 6 7 8 9 10 11 12 13 logging: pattern: dateformat: MM-dd HH:mm:ss:SSS spring: rabbitmq: host: 192.168.79.128 port: 5672 virtual-host: / username: itcast password: 123321 listener: simple: prefetch: 1 #每次只能获取一条消息，处理完成才能获取下一个消息 发布订阅模式 发布（ Publish ）、订阅（ Subscribe ）\n发布订阅模式与之前案例的区别就是允许将同一消息发送给多个消费者。实现方式是加入了exchange（交换机）。\n常见exchange类型包括：\n:::color3 Fanout：广播\nDirect：路由\nTopic：话题\n:::\n注意：exchange负责消息路由，而不是存储，路由失败则消息丢失\n发布订阅-Fanout Exchange 利用SpringAMQP演示FanoutExchange的使用\n实现思路如下：\n在consumer服务中，利用代码声明队列、交换机，并将两者绑定 在consumer服务中，编写两个消费者方法，分别监听fanout.queue1和fanout.queue2 在publisher中编写测试方法，向itcast.fanout发送消息 步骤1：在consumer服务声明Exchange、Queue、Binding\nSpringAMQP提供了声明交换机、队列、绑定关系的API，例如：\n在consumer服务常见一个类，添加@Configuration注解，并声明FanoutExchange、Queue和绑定关系对象Binding，代码如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 @Configuration public class FanoutConfig { // 声明交换机 @Bean public FanoutExchange fanoutExchange() { return new FanoutExchange(\u0026#34;itcast.fanout\u0026#34;); } // 声明队列1 @Bean public Queue fanoutQueue1() { return new Queue(\u0026#34;fanout.queue1\u0026#34;); } // 声明队列2 @Bean public Queue fanoutQueue2() { return new Queue(\u0026#34;fanout.queue2\u0026#34;); } // 将队列1绑定到交换机 @Bean public Binding bindingQueue1ToExchange(Queue fanoutQueue1, FanoutExchange fanoutExchange) { return BindingBuilder.bind(fanoutQueue1).to(fanoutExchange); } // 将队列2绑定到交换机 @Bean public Binding bindingQueue2ToExchange(Queue fanoutQueue2, FanoutExchange fanoutExchange) { return BindingBuilder.bind(fanoutQueue2).to(fanoutExchange); } } 步骤2：在consumer服务声明两个消费者\n在consumer服务的SpringRabbitListener类中，添加两个方法，分别监听fanout.queue1和fanout.queue2：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 /** * 监听发布/订阅模式 广播exchange * * @param message 消息 */ @RabbitListener(queues = \u0026#34;fanout.queue1\u0026#34;) public void listenFanoutQueue1(String message) { System.out.println(\u0026#34;消费者1接收到fanout.queue1消息：\u0026#34; + message); } @RabbitListener(queues = \u0026#34;fanout.queue2\u0026#34;) public void listenFanoutQueue2(String message) { System.out.println(\u0026#34;消费者2接收到fanout.queue2消息：\u0026#34; + message); } 步骤3：在publisher服务发送消息到FanoutExchange\n在publisher服务的SpringAmqpTest类中添加测试方法：\n1 2 3 4 5 6 7 8 9 10 11 12 /** * 测试发送消息到发布/订阅模式 广播exchange */ @Test public void testSendMessageToFanoutExchange() { // 交换机名称 String exchangeName = \u0026#34;itcast.fanout\u0026#34;; // 消息 String message = \u0026#34;hello, fanout message!\u0026#34;; // 发送消息，参数分别是：交互机名称、RoutingKey（暂时为空）、消息 rabbitTemplate.convertAndSend(exchangeName, \u0026#34;\u0026#34;, message); } 效果：\n交换机的作用是什么？\n:::success\n接收publisher发送的消息 将消息按照规则路由到与之绑定的队列 不能缓存消息，路由失败，消息丢失 FanoutExchange的会将消息路由到每个绑定的队列 :::\n声明队列、交换机、绑定关系的Bean是什么？\n:::color2\nQueue FanoutExchange Binding :::\n发布订阅-DirectExchange DirectExchange 会将接收到的消息根据规则路由到指定的Queue，因此称为路由模式（routes）。\n:::success 每一个Queue都与Exchange设置一个BindingKey\n发布者发送消息时，指定消息的RoutingKey\nExchange将消息路由到BindingKey与消息RoutingKey一致的队列\n:::\n利用SpringAMQP演示DirectExchange的使用\n实现思路如下：\n利用@RabbitListener声明Exchange、Queue、RoutingKey 在consumer服务中，编写两个消费者方法，分别监听direct.queue1和direct.queue2 在publisher中编写测试方法，向itcast. direct发送消息 步骤1：在consumer服务声明Exchange、Queue\n1.在consumer服务中，编写两个消费者方法，分别监听direct.queue1和direct.queue2，\n2.并利用@RabbitListener声明Exchange、Queue、RoutingKey\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 /** * 监听发布/订阅模式 路由direct exchange * * @param message 消息 */ @RabbitListener(bindings = @QueueBinding( value = @Queue(value = \u0026#34;direct.queue1\u0026#34;), exchange = @Exchange(value = \u0026#34;direct.exchange\u0026#34;, type = ExchangeTypes.DIRECT), key = {\u0026#34;red\u0026#34;, \u0026#34;blue\u0026#34;} )) public void listenDirectQueue1(String message) { System.out.println(\u0026#34;消费者1接收到direct.queue1消息：\u0026#34; + message); } @RabbitListener(bindings = @QueueBinding( value = @Queue(value = \u0026#34;direct.queue2\u0026#34;), exchange = @Exchange(value = \u0026#34;direct.exchange\u0026#34;, type = ExchangeTypes.DIRECT), key = {\u0026#34;red\u0026#34;, \u0026#34;yellow\u0026#34;} )) public void listenDirectQueue2(String message) { System.out.println(\u0026#34;消费者2接收到direct.queue2消息：\u0026#34; + message); } 步骤2：在publisher服务发送消息到DirectExchange\n在publisher服务的SpringAmqpTest类中添加测试方法：\n1 2 3 4 5 6 7 8 9 10 11 /** * 测试发送消息到发布/订阅模式 路由exchange */ @Test public void testSendMessageToDirectExchange() { // 交换机名称 String exchangeName = \u0026#34;direct.exchange\u0026#34;; rabbitTemplate.convertAndSend(exchangeName, \u0026#34;blue\u0026#34;, \u0026#34;hello, blue direct message!\u0026#34;); rabbitTemplate.convertAndSend(exchangeName, \u0026#34;yellow\u0026#34;, \u0026#34;hello, green direct message!\u0026#34;); rabbitTemplate.convertAndSend(exchangeName, \u0026#34;red\u0026#34;, \u0026#34;hello, yellow direct message!\u0026#34;); } 结果：\n描述下Direct交换机与Fanout交换机的差异？\n:::success Fanout交换机将消息路由给每一个与之绑定的队列\nDirect交换机根据RoutingKey判断路由给哪个队列\n如果多个队列具有相同的RoutingKey，则与Fanout功能类似\n:::\n基于@RabbitListener注解声明队列和交换机有哪些常见注解？\n:::success @Queue\n@Exchange\n:::\n发布订阅-TopicExchange TopicExchange与DirectExchange类似，区别在于routingKey必须是多个单词的列表，并且以 . 分割。\nQueue与Exchange指定BindingKey时可以使用通配符：\n#：代指0个或多个单词\n*：代指一个单词\n利用SpringAMQP演示TopicExchange的使用\n实现思路如下：\n1.并利用@RabbitListener声明Exchange、Queue、RoutingKey\n2.在consumer服务中，编写两个消费者方法，分别监听topic.queue1和topic.queue2\n3.在publisher中编写测试方法，向itcast. topic发送消息\n步骤1：在consumer服务声明Exchange、Queue\n1.在consumer服务中，编写两个消费者方法，分别监听topic.queue1和topic.queue2，\n2.并利用@RabbitListener声明Exchange、Queue、RoutingKey\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 /** * 监听发布/订阅模式 主题topic exchange * * @param message 消息 */ @RabbitListener(bindings = @QueueBinding( value = @Queue(value = \u0026#34;topic.queue1\u0026#34;), exchange = @Exchange(value = \u0026#34;topic.exchange\u0026#34;, type = ExchangeTypes.TOPIC), key = {\u0026#34;china.#\u0026#34;} )) public void listenTopicQueue1(String message) { System.out.println(\u0026#34;消费者1接收到topic.queue1消息 china的所有消息：\u0026#34; + message); } @RabbitListener(bindings = @QueueBinding( value = @Queue(value = \u0026#34;topic.queue2\u0026#34;), exchange = @Exchange(value = \u0026#34;topic.exchange\u0026#34;, type = ExchangeTypes.TOPIC), key = {\u0026#34;#.news\u0026#34;} )) public void listenTopicQueue2(String message) { System.out.println(\u0026#34;消费者2接收到topic.queue2消息 所有news的消息：\u0026#34; + message); } 步骤2：在publisher服务发送消息到TopicExchange\n在publisher服务的SpringAmqpTest类中添加测试方法：\n1 2 3 4 5 6 7 8 9 10 11 /** * 测试发送消息到发布/订阅模式 主题exchange */ @Test public void testSendMessageToTopicExchange() { // 交换机名称 String exchangeName = \u0026#34;topic.exchange\u0026#34;; rabbitTemplate.convertAndSend(exchangeName, \u0026#34;china.news\u0026#34;, \u0026#34;china.news topic message!\u0026#34;); rabbitTemplate.convertAndSend(exchangeName, \u0026#34;china.weather\u0026#34;, \u0026#34;china.weather topic message!\u0026#34;); rabbitTemplate.convertAndSend(exchangeName, \u0026#34;us.news\u0026#34;, \u0026#34;us.news topic message!\u0026#34;); } 结果：\n描述下Direct交换机与Topic交换机的差异？\n:::success Topic交换机接收的消息RoutingKey必须是多个单词，以 . 分割\nTopic交换机与队列绑定时的bindingKey可以指定通配符\n#：代表0个或多个词\n*：代表1个词\n:::\nSpringAMQP-消息转换器 SpringAMQP中消息的序列化和反序列化是怎么实现的？\n:::danger 利用MessageConverter实现的，默认是JDK的序列化\n注意发送方与接收方必须使用相同的MessageConverter\n:::\n在父pom中引入依赖：\n1 2 3 4 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.fasterxml.jackson.core\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jackson-databind\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 在publisher和consumer的启动类中增加配置：\n1 2 3 4 @Bean public MessageConverter messageConverter() { return new Jackson2JsonMessageConverter(); } ","date":"2024-05-07T21:47:36+08:00","image":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A110%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97mq/img/image-32.png","permalink":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A110%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97mq/","title":"【微服务】10、消息队列MQ"},{"content":"为什么需要网关 !](img/image.png)\n网关的技术实现：\n!](img/image-1.png)\n搭建网关服务 搭建网关服务的步骤：\n1.创建新的module，引入SpringCloudGateway的依赖和nacos的服务发现依赖：\n1 2 3 4 5 6 7 8 9 10 \u0026lt;!--网关依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-gateway\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!--nacos服务发现依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-discovery\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 2.编写路由配置及nacos地址\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 server: port: 10010 # 网关端口 spring: application: name: gateway # 服务名称 cloud: nacos: server-addr: localhost:8848 # nacos地址 gateway: routes: # 网关路由配置 - id: user-service # 路由id，自定义，只要唯一即可 # uri: http://127.0.0.1:8081 # 路由的目标地址 http就是固定地址，一般不用这种形式 uri: lb://user-service # 路由的目标地址 lb就是负载均衡，后面跟服务名称 predicates: # 路由断言，也就是判断请求是否符合路由规则的条件 - Path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求 - id: order-service uri: lb://order-service predicates: - Path=/order/** !](img/image-2.png)\n然后访问：\n!](img/image-3.png)\n!](img/image-4.png)\n流程图：\n!](img/image-5.png)\n路由断言工厂Route Predicate Factory 网关路由可以配置的内容包括：\n:::color2\n路由id：路由唯一标示 uri：路由目的地，支持lb和http两种 predicates：路由断言，判断请求是否符合要求，符合则转发到路由目的地 filters：路由过滤器，处理请求或响应 :::\n:::color2\n我们在配置文件中写的断言规则只是字符串，这些字符串会被Predicate Factory读取并处理，转变为路由判断的条件 例如Path=/user/**是按照路径匹配，这个规则是由org.springframework.cloud.gateway.handler.predicate.PathRoutePredicateFactory类来处理的 像这样的断言工厂在SpringCloudGateway还有十几个 :::\nSpring提供了11种基本的Predicate工厂：\n名称 说明 示例 After 是某个时间点后的请求 - After=2037-01-20T17:42:47.789-07:00[America/Denver] Before 是某个时间点之前的请求 - Before=2031-04-13T15:14:47.433+08:00[Asia/Shanghai] Between 是某两个时间点之前的请求 - Between=2037-01-20T17:42:47.789-07:00[America/Denver], 2037-01-21T17:42:47.789-07:00[America/Denver] Cookie 请求必须包含某些cookie - Cookie=chocolate, ch.p Header 请求必须包含某些header - Header=X-Request-Id, \\d+ Host 请求必须是访问某个host（域名） - Host=.somehost.org,.anotherhost.org Method 请求方式必须是指定方式 - Method=GET,POST Path 请求路径必须符合指定规则 - Path=/red/{segment},/blue/** Query 请求参数必须包含指定参数 - Query=name, Jack或者- Query=name RemoteAddr 请求者的ip必须是指定范围 - RemoteAddr=192.168.1.1/24 Weight 权重处理 常用的是Path和Cookie。\n官网地址：[Route Predicate Factories :: Spring Cloud Gateway]!](img/image-6.png)\n路由过滤器 GatewayFilter 官网地址 ：[GatewayFilter Factories :: Spring Cloud Gateway]!](img/image-7.png)\nSpring提供了30+种不同的路由过滤器工厂。例如：\n名称 说明 AddRequestHeader 给当前请求添加一个请求头 RemoveRequestHeader 移除请求中的一个请求头 AddResponseHeader 给响应结果中添加一个响应头 RemoveResponseHeader 从响应结果中移除有一个响应头 RequestRateLimiter 限制请求的流量 \u0026hellip; !](img/image-8.png)\n如果要对所有的路由都生效，则可以将过滤器工厂写到default下。格式如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 server: port: 10010 # 网关端口 spring: application: name: gateway # 服务名称 cloud: nacos: server-addr: localhost:8848 # nacos地址 gateway: routes: # 网关路由配置 - id: user-service # 路由id，自定义，只要唯一即可 # uri: http://127.0.0.1:8081 # 路由的目标地址 http就是固定地址，一般不用这种形式 uri: lb://user-service # 路由的目标地址 lb就是负载均衡，后面跟服务名称 predicates: # 路由断言，也就是判断请求是否符合路由规则的条件 - Path=/user/** # 这个是按照路径匹配，只要以/user/开头就符合要求 filters: # 过滤器，可以对请求进行一些处理 - AddRequestHeader=X-Request-Red, blue # 添加请求头，key是X-Request-Red，value是blue - id: order-service uri: lb://order-service predicates: - Path=/order/** default-filters: # 默认过滤器，会对所有的路由请求都生效 - AddRequestHeader=X-Request-Default, default 全局过滤器 GlobalFilter 全局过滤器的作用也是处理一切进入网关的请求和微服务响应，与GatewayFilter的作用一样。\n区别在于GatewayFilter通过配置定义，处理逻辑是固定的。而GlobalFilter的逻辑需要自己写代码实现。\n!](img/image-9.png)\n定义方式是实现GlobalFilter接口。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 // @Order(-1) // 优先级, 数字越小, 优先级越高，这里用实现Ordered接口的方式代替 @Component public class AuthorizeFilter implements GlobalFilter, Ordered { /** * 处理当前请求，有必要的话通过{@link GatewayFilterChain}将请求交给下一个过滤器处理 * * @param exchange 请求上下文，里面可以获取Request、Response等信息 * @param chain 过滤器链，用来把请求委托给下一个过滤器 * @return {@code Mono\u0026lt;Void\u0026gt;} 返回标示当前过滤器业务结束 */ @Override public Mono\u0026lt;Void\u0026gt; filter(ServerWebExchange exchange, GatewayFilterChain chain) { // 1.获取请求 ServerHttpRequest request = exchange.getRequest(); // 2.获取请求参数 String authorization = request.getQueryParams().getFirst(\u0026#34;authorization\u0026#34;); // 3.判断是否有authorization参数 if (\u0026#34;admin\u0026#34;.equals(authorization)) { // 4.有, 放行 return chain.filter(exchange); } // 5.没有, 设置状态码返回401 exchange.getResponse().setStatusCode(HttpStatus.UNAUTHORIZED); return exchange.getResponse().setComplete(); } @Override public int getOrder() { return -1; } } 过滤器执行顺序 请求进入网关会碰到三类过滤器：当前路由的过滤器、DefaultFilter、GlobalFilter\n请求路由后，会将当前路由过滤器和DefaultFilter、GlobalFilter，合并到一个过滤器链（集合）中，排序后依次执行每个过滤器\n!](img/image-10.png)\n:::color4\n每一个过滤器都必须指定一个int类型的order值，order值越小，优先级越高，执行顺序越靠前。 GlobalFilter通过实现Ordered接口，或者添加@Order注解来指定order值，由我们自己指定 路由过滤器和defaultFilter的order由Spring指定，默认是按照声明顺序从1递增。 当过滤器的order值一样时，会按照 defaultFilter \u0026gt; 路由过滤器 \u0026gt; GlobalFilter的顺序执行。 :::\n可以参考下面几个类的源码来查看：\norg.springframework.cloud.gateway.route.RouteDefinitionRouteLocator#getFilters()方法是先加载defaultFilters，然后再加载某个route的filters，然后合并。\norg.springframework.cloud.gateway.handler.FilteringWebHandler#handle()方法会加载全局过滤器，与前面的过滤器合并后根据order排序，组织过滤器链\n跨域问题处理 跨域：域名不一致就是跨域，主要包括：\n域名不同： www.taobao.com 和 www.taobao.org 和 www.jd.com 和 miaosha.jd.com 域名相同，端口不同：localhost:8080和localhost8081 跨域问题：浏览器禁止请求的发起者与服务端发生跨域ajax请求，请求被浏览器拦截的问题\n解决方案：CORS CORS\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 server: port: 10010 # 网关端口 spring: application: name: gateway # 服务名称 cloud: nacos: server-addr: localhost:8848 # nacos地址 gateway: ... globalcors: add-to-simple-url-handler-mapping: true # 解决options请求被拦截问题 corsConfigurations: \u0026#39;[/**]\u0026#39;: # 所有的请求都生效 allowedOrigins: # 允许哪些网站跨域请求 - \u0026#34;http://localhost:8080\u0026#34; # 允许的请求来源 - \u0026#34;http://www.baidu.com\u0026#34; # 允许的请求来源 allowedMethods: # 允许哪些请求方法 - GET - POST - PUT - DELETE - OPTIONS allowedHeaders: \u0026#34;*\u0026#34; # 允许在请求头中携带的信息 allowCredentials: true # 是否允许携带cookie maxAge: 360000 # 这次跨域检测的有效期，为了减少检测次数，提高性能 ","date":"2024-04-28T21:38:29+08:00","permalink":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A19%E7%BB%9F%E4%B8%80%E7%BD%91%E5%85%B3gateway/","title":"【微服务】9、统一网关Gateway"},{"content":"定义和使用Feign客户端 Feign的介绍\nFeign是一个声明式的http客户端，官方地址：https://github.com/OpenFeign/feign\n其作用就是帮助我们优雅的实现http请求的发送，解决上面提到的问题。\n使用Feign客户端\n引入依赖 1 2 3 4 5 \u0026lt;!-- OpenFeign --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-openfeign\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 在order-service的启动类添加注解开启Feign的功能 编写Feign客户端 1 2 3 4 5 @FeignClient(\u0026#34;userservice\u0026#34;) public interface UserClient { @GetMapping(\u0026#34;/user/{id}\u0026#34;) User findById(@PathVariable(\u0026#34;id\u0026#34;) Long id); } 主要是基于SpringMVC的注解来声明远程调用的信息，比如：\n服务名称：userservice 请求方式：GET 请求路径：/user/{id} 请求参数：Long id 返回值类型：User 用Feign客户端代替RestTemplate 自定义Feign的配置 Feign运行自定义配置来覆盖默认配置，可以修改的配置如下：\n类型 作用 说明 feign.Logger.Level 修改日志级别 包含四种不同的级别：NONE、BASIC、HEADERS、FULL feign.codec.Decoder 响应结果的解析器 http远程调用的结果做解析，例如解析json字符串为java对象 feign.codec.Encoder 请求参数编码 将请求参数编码，便于通过http请求发送 feign. Contract 支持的注解格式 默认是SpringMVC的注解 feign. Retryer 失败重试机制 请求失败的重试机制，默认是没有，不过会使用Ribbon的重试 一般我们需要配置的就是日志级别。\n配置Feign日志有两种方式：\n方式一：配置文件方式 全局生效：\n1 2 3 4 5 feign: client: config: default: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置 loggerLevel: FULL # 日志级别 局部生效：\n1 2 3 4 5 feign: client: config: userservice: # 这里用default就是全局配置，如果是写服务名称，则是针对某个微服务的配置 loggerLevel: FULL # 日志级别 方式二：java代码方式，需要先声明一个Bean： 1 2 3 4 5 6 public class FeignClientConfiguration { @Bean public Logger.Level feignLogLevel(){ return Logger.Level.BASIC; } } 而后如果是全局配置，则把它放到@EnableFeignClients这个注解中：\n1 @EnableFeignClients(defaultConfiguration = FeignClientConfiguration.class) 如果是局部配置，则把它放到@FeignClient这个注解中\n1 @FeignClient(value = \u0026#34;userservice\u0026#34;, configuration = FeignClientConfiguration.class) Feign的性能优化 Feign底层的客户端实现：\nURLConnection：默认实现，不支持连接池 Apache HttpClient：支持连接池 OKHttp：支持连接池 因此优化Feign的性能主要包括：\n①使用连接池代替默认的URLConnection\n②日志级别，最好用basic或none\nFeign添加HttpClient的支持：\n引入依赖：\n1 2 3 4 5 \u0026lt;!--httpClient的依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;io.github.openfeign\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;feign-httpclient\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 配置连接池：\n1 2 3 4 5 6 7 8 9 feign: client: config: default: # default全局的配置 loggerLevel: BASIC # 日志级别，BASIC就是基本的请求和响应信息 httpclient: enabled: true # 开启feign对HttpClient的支持 max-connections: 200 # 最大的连接数 max-connections-per-route: 50 # 每个路径的最大连接数 Feign的最佳实践 方式一（继承） 给消费者的FeignClient和提供者的controller定义统一的父接口作为标准。\n不推荐：因为紧耦合和spring MVC的参数映射无法继承\n方式二（抽取） 将FeignClient抽取为独立模块，并且把接口有关的POJO、默认的Feign配置都放到这个模块中，提供给所有消费者使用\n实现最佳实践方式二的步骤如下：\n首先创建一个module，命名为feign-api，然后引入feign的starter依赖 将order-service中编写的UserClient、User、DefaultFeignConfiguration都复制到feign-api项目中 order-service中引入feign-api的依赖 修改order-service中的所有与上述三个组件有关的import部分，改成导入feign-api中的包 重启测试 此时会启动失败：\n这是因为springboot的启动扫描默认不会扫描到这个bean，解决办法：\n当定义的FeignClient不在SpringBootApplication的扫描包范围时，这些FeignClient无法使用。有两种方式解决：\n方式一：指定FeignClient所在包\n方式二：指定FeignClient字节码\n","date":"2024-04-27T21:34:29+08:00","permalink":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A18http%E5%AE%A2%E6%88%B7%E7%AB%AFfeign/","title":"【微服务】8、http客户端Feign"},{"content":"统一配置管理 在Nacos中添加配置信息：\n在弹出表单中填写配置信息：\n配置获取的步骤如下：\n引入Nacos的配置管理客户端依赖： 1 2 3 4 5 \u0026lt;!--nacos配置管理依赖--\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-config\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 在userservice中的resource目录添加一个bootstrap.yml文件，这个文件是引导文件，优先级高于application.yml： 1 2 3 4 5 6 7 8 9 10 spring: application: name: user-service # 服务名称 profiles: active: dev #开发环境，这里是dev cloud: nacos: server-addr: localhost:8848 # Nacos地址 config: file-extension: yaml # 文件后缀名 我们在user-service中将pattern.dateformat这个属性注入到UserController中做测试：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 @RestController @RequestMapping(\u0026#34;/user\u0026#34;) public class UserController { // 注入nacos中的配置属性 @Value(\u0026#34;${pattern.dateformat}\u0026#34;) private String dateformat; // 编写controller，通过日期格式化器来格式化现在时间并返回 @GetMapping(\u0026#34;now\u0026#34;) public String now(){ return LocalDate.now().format( DateTimeFormatter.ofPattern(dateformat, Locale.CHINA) ); } // ... 略 } 结果：\n总结将配置交给Nacos管理的步骤 :::info ①在Nacos中添加配置文件\n②在微服务中引入nacos的config依赖\n③在微服务中添加bootstrap.yml，配置nacos地址、当前环境、服务名称、文件后缀名。这些决定了程序启动时去nacos读取哪个文件\n:::\n配置自动刷新 Nacos中的配置文件变更后，微服务无需重启就可以感知。不过需要通过下面两种配置实现：\n方式一：在@Value注入的变量所在类上添加注解@RefreshScope\n方式二：使用@ConfigurationProperties注解\n然后在要使用配置的地方注入PatternProperties就可以使用了\nNacos配置更改后，微服务可以实现热更新，方式：\n:::warning ①通过@Value注解注入，结合@RefreshScope来刷新\n②通过@ConfigurationProperties注入，自动刷新\n:::\n注意事项：\n•不是所有的配置都适合放到配置中心，维护起来比较麻烦\n•建议将一些关键参数，需要运行时调整的参数放到nacos配置中心，一般都是自定义配置\n多环境配置共享 微服务启动时会从nacos读取多个配置文件：\n[spring.application.name]-[spring.profiles.active].yaml，例如：userservice-dev.yaml [spring.application.name].yaml，例如：userservice.yaml 无论profile如何变化，[spring.application.name].yaml这个文件一定会加载，因此多环境共享配置可以写入这个文件\n优先级：\n服务名-profile.yaml \u0026gt;服务名称.yaml \u0026gt; 本地配置\n不常用：不同微服务之间可以共享配置文件，通过下面的两种方式来指定：\n方式一：\n1 2 3 4 5 6 7 8 9 10 11 12 spring: application: name: userservice # 服务名称 profiles: active: dev # 环境， cloud: nacos: server-addr: localhost:8848 # Nacos地址 config: file-extension: yaml # 文件后缀名 shared-configs: # 多微服务间共享的配置列表 - dataId: common.yaml # 要共享的配置文件id 方式二：\n1 2 3 4 5 6 7 8 9 10 11 12 spring: application: name: userservice # 服务名称 profiles: active: dev # 环境， cloud: nacos: server-addr: localhost:8848 # Nacos地址 config: file-extension: yaml # 文件后缀名 extends-configs: # 多微服务间共享的配置列表 - dataId: extend.yaml # 要共享的配置文件id 多种配置的优先级：\n服务名-profile.yaml \u0026gt;服务名称.yaml \u0026gt; extension-config \u0026gt; shared-config \u0026gt; 本地配置\n","date":"2024-04-24T21:30:01+08:00","permalink":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A17nacos%E9%85%8D%E7%BD%AE%E7%AE%A1%E7%90%86/","title":"【微服务】7、Nacos配置管理"},{"content":"nacos服务分级 nacos引入了服务-\u0026gt;集群-\u0026gt;实例的分级概念\n服务集群属性\n1.修改application.yml，添加如下内容：\n1 2 3 4 5 6 spring: cloud: nacos: server-addr: localhost:8848 # nacos 服务端地址 discovery: cluster-name: HZ # 配置集群名称，也就是机房位置，例如：HZ，杭州 然后启动项目\n2.在Nacos控制台可以看到集群变化：\nnacos负载均衡 修改order-service中的application.yml，设置集群为HZ： 1 2 3 4 5 6 spring: cloud: nacos: server-addr: localhost:8848 # nacos 服务端地址 discovery: cluster-name: HZ # 配置集群名称，也就是机房位置 然后在order-service中设置负载均衡的IRule为NacosRule，这个规则优先会寻找与自己同集群的服务： 1 2 3 userservice: ribbon: NFLoadBalancerRuleClassName: com.alibaba.cloud.nacos.ribbon.NacosRule# 负载均衡规则 NacosRule负载均衡策略 :::color2 ①优先选择同集群服务实例列表\n②本地集群找不到提供者，才去其它集群寻找，并且会报警告\n③确定了可用实例列表后，再采用随机负载均衡挑选实例\n:::\n根据权重负载均衡 实际部署中会出现这样的场景：\n服务器设备性能有差异，部分实例所在机器性能较好，另一些较差，我们希望性能好的机器承担更多的用户请求\nNacos提供了权重配置来控制访问频率，权重越大则访问频率越高\n在Nacos控制台可以设置实例的权重值，首先选中实例后面的编辑按钮 将权重设置为0.1，测试可以发现8081被访问到的频率大大降低 总结 实例的权重控制\n:::color2 ①Nacos控制台可以设置实例的权重值，0~1之间\n②同集群内的多个实例，权重越高被访问的频率越高\n③权重设置为0则完全不会被访问，可以以此来做灰度发布\n:::\n环境隔离 - namespace 在Nacos控制台可以创建namespace，用来隔离不同环境 然后填写一个新的命名空间信息： 保存后会在控制台看到这个命名空间的id 修改order-service的application.yml，添加namespace 1 2 3 4 5 6 7 spring: cloud: nacos: server-addr: localhost:8848 discovery: cluster-name: SH # 上海 namespace: 492a7d5d-237b-46a1-a99a-fa8e98e4b0f9 # 命名空间，填ID 重启order-service后，再来查看控制台 此时访问order-service，因为namespace不同，会导致找不到userservice，控制台会报错 总结 :::warning ①每个namespace都有唯一id\n②服务设置namespace时要写id而不是名称\n③不同namespace下的服务互相不可见\n:::\nnacos注册中心细节分析 服务注册到Nacos时，可以选择注册为临时或非临时实例，通过下面的配置来设置：\n1 2 3 4 5 spring: cloud: nacos: discovery: ephemeral: false # 设置为非临时实例 临时实例宕机时，会从nacos的服务列表中剔除，而非临时实例则不会\n总结nacos与eureka的异同 :::warning Nacos与eureka的共同点\n①都支持服务注册和服务拉取\n②都支持服务提供者心跳方式做健康检测\n:::\n:::color1 Nacos与Eureka的区别\n①Nacos支持服务端主动检测提供者状态：临时实例采用心跳模式，非临时实例采用主动检测模式\n②临时实例心跳不正常会被剔除，非临时实例则不会被剔除\n③Nacos支持服务列表变更的消息推送模式，服务列表更新更及时\n④Nacos集群默认采用AP方式，当集群中存在非临时实例时，采用CP模式；Eureka采用AP方式\n:::\nAP（可用性和分区容忍性）：\n在AP模式下，Nacos追求高可用性。即使在网络分区的情况下，Nacos仍然会继续提供服务，即使在数据之间可能存在一定的不一致。这种模式适用于对可用性要求较高、可以容忍一定数据不一致的场景，例如大多数互联网应用。\nCP（一致性和分区容忍性）：\n在CP模式下，Nacos追求强一致性。这意味着在面临网络分区时，Nacos会牺牲一定的可用性，以保持数据的一致性。这种模式适用于对一致性要求较高的场景，例如金融、电商等对数据准确性要求高的领域。\n","date":"2024-04-23T21:19:29+08:00","permalink":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A16nacos%E6%9C%8D%E5%8A%A1%E5%88%86%E7%BA%A7%E5%AD%98%E5%82%A8%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/","title":"【微服务】6、Nacos服务分级存储和负载均衡"},{"content":"认识和安装nacos Nacos是阿里巴巴的产品，现在是SpringCloud中的一个组件。相比Eureka功能更加丰富，在国内受欢迎程度较高。\nWindows安装 开发阶段采用单机安装即可。\n下载安装包 在Nacos的GitHub页面，提供有下载链接，可以下载编译好的Nacos服务端或者源代码：\nGitHub主页：https://github.com/alibaba/nacos\nGitHub的Release下载页：https://github.com/alibaba/nacos/releases\n如图：\nwindows版本使用nacos-server-1.4.1.zip包即可。\n解压 将这个包解压到任意非中文目录下，如图：\n目录说明：\n:::color2\nbin：启动脚本 conf：配置文件 :::\n端口配置 Nacos的默认端口是8848，如果你电脑上的其它进程占用了8848端口，请先尝试关闭该进程。\n如果无法关闭占用8848端口的进程，也可以进入nacos的conf目录，修改配置文件中的端口：\n启动 启动非常简单，进入bin目录，结构如下：\n然后执行命令即可：\nwindows命令： 1 ./startup.cmd -m standalone 执行后的效果如图：\n访问 在浏览器输入地址：http://127.0.0.1:8848/nacos即可：\n默认的账号和密码都是nacos，进入后：\nLinux安装 Linux或者Mac安装方式与Windows类似。\n安装JDK Nacos依赖于JDK运行，索引Linux上也需要安装JDK才行。\n上传jdk安装包：\n上传到某个目录，例如：/usr/local/\n然后解压缩：\n1 tar -xvf jdk-8u144-linux-x64.tar.gz 然后重命名为java\n配置环境变量：\n1 2 export JAVA_HOME=/usr/local/java export PATH=$PATH:$JAVA_HOME/bin 设置环境变量：\n1 source /etc/profile 上传安装包 如图：\n上传.tar.gz压缩包到Linux服务器的某个目录，例如/usr/local/src目录下：\n解压 命令解压缩安装包：\n1 tar -xvf nacos-server-1.4.1.tar.gz 然后删除安装包：\n1 rm -rf nacos-server-1.4.1.tar.gz 目录中最终样式：\n目录内部：\n端口配置 与windows中类似\n启动 在nacos/bin目录中，输入命令启动Nacos：\n1 sh startup.sh -m standalone Nacos的依赖 父工程：\n1 2 3 4 5 6 7 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-alibaba-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.5.RELEASE\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 客户端：\n1 2 3 4 5 \u0026lt;!-- nacos客户端依赖包 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-discovery\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 服务注册到nacos 在cloud-demo父工程中添加spring-cloud-alilbaba的管理依赖： 1 2 3 4 5 6 7 \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-alibaba-dependencies\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.6.RELEASE\u0026lt;/version\u0026gt; \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt; \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; 注释掉order-service和user-service中原有的eureka依赖。 添加nacos的客户端依赖： 1 2 3 4 5 \u0026lt;!-- nacos客户端依赖 --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.alibaba.cloud\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;spring-cloud-starter-alibaba-nacos-discovery\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; 修改user-service\u0026amp;order-service中的application.yml文件，注释eureka地址，添加nacos地址： 1 2 3 4 spring: cloud: nacos: server-addr: localhost:8848 # nacos 服务端地址 启动并测试： 总结 ","date":"2024-04-22T21:07:42+08:00","permalink":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A15nacos%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/","title":"【微服务】5、Nacos注册中心"},{"content":"负载均衡流程 负载均衡策略 内置负载均衡规则类 规则描述 RoundRobinRule 简单轮询服务列表来选择服务器。它是Ribbon默认的负载均衡规则。 AvailabilityFilteringRule 对以下两种服务器进行忽略：（1）在默认情况下，这台服务器如果3次连接失败，这台服务器就会被设置为“短路”状态。短路状态将持续30秒，如果再次连接失败，短路的持续时间就会几何级地增加。（2）并发数过高的服务器。如果一个服务器的并发连接数过高，配置了AvailabilityFilteringRule规则的客户端也会将其忽略。并发连接数的上限，可以由客户端的\u0026laquo;/font\u0026gt;clientName\u0026gt;.\u0026laquo;/font\u0026gt;clientConfigNameSpace\u0026gt;.ActiveConnectionsLimit属性进行配置。 WeightedResponseTimeRule 为每一个服务器赋予一个权重值。服务器响应时间越长，这个服务器的权重就越小。这个规则会随机选择服务器，这个权重值会影响服务器的选择。 ZoneAvoidanceRule 以区域可用的服务器为基础进行服务器的选择。使用Zone对服务器进行分类，这个Zone可以理解为一个机房、一个机架等。而后再对Zone内的多个服务做轮询。 BestAvailableRule 忽略那些短路的服务器，并选择并发数较低的服务器。 RandomRule 随机选择一个可用的服务器。 RetryRule 重试机制的选择逻辑 注意：第一种方式为全局设置（order服务访问任何微服务都为随机策略），第二种方法可以针对单个微服务设置负载均衡\n饥饿加载 Ribbon默认是采用懒加载，即第一次访问时才会去创建LoadBalanceClient，请求时间会很长。\n而饥饿加载则会在项目启动时创建，降低第一次访问的耗时，通过下面配置开启饥饿加载：\n1 2 3 4 ribbon: eager-load: enabled: true # 开启饥饿加载 clients: userservice # 指定对userservice这个服务饥饿加载 总结 ","date":"2024-04-17T15:33:50+08:00","permalink":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A14ribbon%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/","title":"【微服务】4、Ribbon负载均衡"},{"content":"服务调用出现的问题 :::color3\n服务消费者该如何获取服务提供者的地址信息？ 果有多个服务提供者，消费者该如何选择？ 消费者如何得知服务提供者的健康状态？ :::\nEureka的作用 消费者该如何获取服务提供者具体信息？\n服务提供者启动时向eureka注册自己的信息\neureka保存这些信息\n消费者根据服务名称向eureka拉取提供者信息\n如果有多个服务提供者，消费者该如何选择？\n服务消费者利用负载均衡算法，从服务列表中挑选一个\n消费者如何感知服务提供者健康状态？\n服务提供者会每隔30秒向EurekaServer发送心跳请求，报告健康状态\neureka会更新记录服务列表信息，心跳不正常会被剔除\n消费者就可以拉取到最新的信息\n总结 ","date":"2024-04-16T16:23:35+08:00","permalink":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A13eureka%E6%B3%A8%E5%86%8C%E4%B8%AD%E5%BF%83/","title":"【微服务】3、Eureka注册中心"},{"content":"微服务拆分 cloud-order.sqlcloud-user.sql\n提供者与消费者 :::warning\n服务提供者：一次业务中，被其它微服务调用的服务。（提供接口给其它微服务） 服务消费者：一次业务中，调用其它微服务的服务。（调用其它微服务提供的接口） :::\n服务调用关系\n:::color2\n服务提供者：暴露接口给其它微服务调用 服务消费者：调用其它微服务提供的接口 提供者与消费者角色其实是相对的 一个服务可以同时是服务提供者和服务消费者 :::\n","date":"2024-04-16T15:33:50+08:00","permalink":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A12%E6%9C%8D%E5%8A%A1%E6%8B%86%E5%88%86%E5%8F%8A%E8%BF%9C%E7%A8%8B%E8%B0%83%E7%94%A8/","title":"【微服务】2、服务拆分及远程调用"},{"content":"服务架构演变 微服务是一种经过良好架构设计的分布式架构方案，微服务架构特征：\n单一职责：微服务拆分粒度更小，每一个服务都对应唯一的业务能力，做到单一职责，避免重复业务开发 面向服务：微服务对外暴露业务接口 自治：团队独立、技术独立、数据独立、部署独立 隔离性强：服务调用做好隔离、容错、降级，避免出现级联问题 微服务技术对比 SpringCloud SpringCloud是目前国内使用最广泛的微服务框架。官网地址：https://spring.io/projects/spring-cloud。 SpringCloud集成了各种微服务功能组件，并基于SpringBoot实现了这些组件的自动装配，从而提供了良好的开箱即用体验： lSpringCloud与SpringBoot的版本兼容关系可以在官网找到 ：\n我们用的版本是 Hoxton.SR10，因此对应的SpringBoot版本是2.3.x版本。\n","date":"2024-04-16T15:24:26+08:00","permalink":"https://xxl999227.github.io/archives/%E5%BE%AE%E6%9C%8D%E5%8A%A11%E8%AE%A4%E8%AF%86%E5%BE%AE%E6%9C%8D%E5%8A%A1/","title":"【微服务】1、认识微服务"},{"content":"第一章：Kubernetes 集群架构 Docker 是每一个节点（包括 Master 节点和 Node 节点）的运行时环境（也有Containerd）。 kubelet 负责控制所有容器的启动和停止等，保证每个节点（包括 Master 节点和 Node 节点）正常工作，并且帮助 Node 节点和 Master 节点进行交互。 Master 节点的关键组件： kubelet（监工）：所有节点必备的。控制当前节点所有 Pod 的生命周期以及与 api-server 交互等工作。 kube-api-server：负责接收所有请求。集群内对集群的任何修改都是通过命令行、UI 将请求发给 api-server 才能执行的。api-server 是整个集群操作对内、对外的唯一入口，不包含我们后来部署应用暴露端口的方式。 kube-proxy：整个节点的网络流量负责。 cri：容器运行时环境（如：Docker 、Podman 等）。 …… Node 节点的关键组件： kubelet（监工）：所有节点必备的。控制当前节点所有 Pod 的生命周期以及与 api-server 交互等工作。 kube-proxy：整个节点的网络流量负责。 cri：容器运行时环境（如：Docker 、Podman 等）。 第二章：资源管理方式 概述 ① 命令式对象管理：直接通过命令去操作 Kubernetes 的资源。 1 kubectl run nginx-pod --image=nginx:1.17.1 --port=80 ② 命令式对象配置：通过命令配置和配置文件去操作 Kubernetes 的资源。 1 kubectl create/patch/delete -f nginx-pod.yaml ③ 声明式对象配置：通过 apply 命令和配置文件去操作 Kubernetes 的资源。 1 kubectl apply -f nginx-pod.yaml 总结： 类型 操作 适用场景 优点 缺点 命令式对象管理 对象 测试 简单 只能操作活动对象，无法审计、跟踪 命令式对象配置 文件 开发 可以审计、跟踪 项目大的时候，配置文件多，操作麻烦 声明式对象配置 目录 开发 支持目录操作 意外情况下难以调试 命令式对象管理 kubectl 命令 kubectl 是 Kubernetes 集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装和部署。 kubectl 命令的语法如下： 1 kubectl [command] [type] [name] [flags] 参数： command：指定要对资源执行的操作，如：create、get 、delete 等。 type：指定资源的类型，如：deployment 、pod 、service 等。 name：指定资源的名称，名称大小写敏感。 flags：指定额外的可选参数。 示例：查看所有的 Pod 1 kubectl get pod 示例：以 yaml 格式查看某个 Pod 1 kubectl get pod 《pod_name》 -o yaml 操作（command） Kubernetes 允许对资源进行多种操作，可以通过 \u0026ndash;help 查看详细的操作命令： 1 kubectl --help 经常使用的操作如下所示： ① 基本命令： 命令 翻译 命令作用 create 创建 创建一个资源 edit 编辑 编辑一个资源 get 获取 获取一个资源 patch 更新 更新一个资源 delete 删除 删除一个资源 explain 解释 展示资源文档 ② 运行和调试： 命令 翻译 命令作用 run 运行 在集群中运行一个指定的镜像 expose 暴露 暴露资源为 Service describe 描述 显示资源内部信息 logs 日志 输出容器在 Pod 中的日志 attach 缠绕 进入运行中的容器 exec 执行 执行容器中的一个命令 cp 复制 在 Pod 内外复制文件 rollout 首次展示 管理资源的发布 scale 规模 扩（缩）容 Pod 的数量 autoscale 自动调整 自动调整 Pod 的数量 ③ 高级命令： 命令 翻译 命令作用 apply 应用 通过文件对资源进行配置 label 标签 更新资源上的标签 ④ 其他命令： 命令 翻译 命令作用 cluster-info 集群信息 显示集群信息 version 版本 显示当前 Client 和 Server 的版本 资源类型（type） Kubernetes 中所有的内容都抽象为资源，可以通过下面的命令进行查看： 1 kubectl api-resources 经常使用的资源如下所示： ① 集群级别资源： 资源名称 缩写 资源作用 nodes no 集群组成部分 namespaces ns 隔离 Pod ② Pod资源： 资源名称 缩写 资源作用 Pods po 装载容器 ③ Pod资源控制器： 资源名称 缩写 资源作用 replicationcontrollers rc 控制 Pod 资源 replicasets rs 控制 Pod 资源 deployments deploy 控制 Pod 资源 daemonsets ds 控制 Pod 资源 jobs 控制 Pod 资源 cronjobs cj 控制 Pod 资源 horizontalpodautoscalers hpa 控制 Pod 资源 statefulsets sts 控制 Pod 资源 ④ 服务发现资源： 资源名称 缩写 资源作用 services svc 统一 Pod 对外接口 ingress ing 统一 Pod 对外接口 ⑤ 存储资源： 资源名称 缩写 资源作用 volumeattachments 存储 persistentvolumes pv 存储 persistentvolumeclaims pvc 存储 ⑥ 配置资源： 资源名称 缩写 资源作用 configmaps cm 配置 secrets 配置 示例：查询命名空间 1 kubectl get ns 示例：创建命名空间 1 kubectl create ns dev 示例：删除命名空间 1 kubectl delete ns dev 命令式对象配置 命令式对象配置就是通过命令配置和配置文件去操作 Kubernetes 的资源。 命令式对象配置的方式操作资源，可以简单的认为：命令 + yaml 配置文件（里面是命令需要的各种参数）。 示例： ① 创建一个 nginxpod.yaml 文件，内容如下（第五行的三横杠 \u0026mdash; 是yaml的结束标志，后面的可以理解为另一个yaml了）： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Namespace metadata: name: dev --- apiVersion: v1 kind: Pod metadata: name: nginxpod namespace: dev spec: containers: - name: nginx-containers image: nginx:1.17.1 ② 执行 create 命令，创建资源： 1 kubectl create -f nginxpod.yaml ③ 执行 get 命令，查看资源： 1 kubectl get -f nginxpod.yaml ④ 执行 delete 命令，删除资源： 1 kubectl delete -f nginxpod.yaml 声明式对象配置 声明式对象配置：通过 apply 命令和配置文件去操作 Kubernetes 的资源。\n声明式对象配置和命令式对象配置类似，只不过它只有一个 apply 命令。\napply 命令相当于 create 命令和 patch 命令。\n示例：\n① 创建一个 nginxpod.yaml 文件，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Namespace metadata: name: dev --- apiVersion: v1 kind: Pod metadata: name: nginxpod namespace: dev spec: containers: - name: nginx-containers image: nginx:1.17.1 ② 执行 apply 命令： 1 kubectl apply -f nginxpod.yaml 总结 创建和更新资源使用声明式对象配置：kubectl apply -f xxx.yaml。 删除资源使用命令式对象配置：kubectl delete -f xxx.yaml。 查询资源使用命令式对象管理：kubectl get(describe) 资源名称。 故障排除（必须记住） 显示资源列表 命令： 1 kubectl get 资源类型 示例：获取类型为 Deployment 的资源列表 1 kubectl get deployment 示例：获取类型为 Pod 的资源列表 1 kubectl get pod 示例：获取类型为 Node 的资源列表 1 kubectl get node 示例：显示所有名称空间下的 Deployment 1 kubectl get deployment -A 1 kubectl get deployment --all-namespaces 示例：查看 kube-system 名称空间下的 Deployment 1 kubectl get deployment -n kube-system 示例：查看在名称空间下的资源对象 1 kubectl api-resources --namespaced=true 示例：查询不在名称空间下的资源对象（理解：就是受 Kubernetes 集群统一调度，而不受到具体名称空间的约束） 1 kubectl api-resources --namespaced=false 显示有关资源的详细信息 命令： 1 kubectl describe 资源类型 资源名称 示例：查询名称为 nginx-pod 的 Pod 的信息 1 kubectl describe pod nginxpod -n dev 示例：显示名称为 nginx 的 Deployment 的信息 1 kubectl describe deployment nginx 查询 Pod 中的容器的打印日志 命令：类似于 Docker 的 docker logs -f xxx 1 kubectl logs -f Pod名称 示例：查询名称为 nginxpod 的 Pod 日志 1 kubectl logs -f nginxpod -n dev 在 Pod 中的容器环境内执行命令 命令：类似于 Docker 的 docker exec -it xxx /bin/bash 1 kubectl exec -it xxx -- /bin/bash 示例：在名称为 nginx-pod 的容器中执行命令 1 kubectl exec -it nginx-pod -- /bin/bash 第三章：实战入门 Namespace Namespace是kubernetes系统中的一种非常重要资源，它的主要作用是用来实现多套环境的资源隔离或者多租户的资源隔离。 默认情况下，kubernetes集群中的所有的Pod都是可以相互访问的。但是在实际中，可能不想让两个Pod之间进行互相的访问，那此时就可以将两个Pod划分到不同的namespace下。kubernetes通过将集群内部的资源分配到不同的Namespace中，可以形成逻辑上的\u0026quot;组\u0026quot;，以方便不同的组的资源进行隔离使用和管理。 可以通过kubernetes的授权机制，将不同的namespace交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合kubernetes的资源配额机制，限定不同租户能占用的资源，例如CPU使用量、内存使用量等等，来实现租户可用资源的管理。 namespace资源隔离、网络不隔离，如：配置文件不可以跨namespace访问，但是网络访问可以跨名称空间访问。 kubernetes在集群启动之后，会默认创建几个namespace\n1 2 3 4 5 6 [root@master ~]# kubectl get namespace NAME STATUS AGE default Active 45h # 所有未指定Namespace的对象都会被分配在default命名空间 kube-node-lease Active 45h # 集群节点之间的心跳维护，v1.13开始引入 kube-public Active 45h # 此命名空间下的资源可以被所有人访问（包括未认证用户） kube-system Active 45h # 所有由Kubernetes系统创建的资源都处于这个命名空间 下面来看namespace资源的具体操作：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 # 1 查看所有的ns 命令：kubectl get ns [root@master ~]# kubectl get ns NAME STATUS AGE default Active 45h kube-node-lease Active 45h kube-public Active 45h kube-system Active 45h # 2 查看指定的ns 命令：kubectl get ns ns名称 [root@master ~]# kubectl get ns default NAME STATUS AGE default Active 45h # 3 指定输出格式 命令：kubectl get ns ns名称 -o 格式参数 # kubernetes支持的格式有很多，比较常见的是wide、json、yaml [root@master ~]# kubectl get ns default -o yaml apiVersion: v1 kind: Namespace metadata: creationTimestamp: \u0026#34;2020-04-05T04:44:16Z\u0026#34; name: default resourceVersion: \u0026#34;151\u0026#34; selfLink: /api/v1/namespaces/default uid: 7405f73a-e486-43d4-9db6-145f1409f090 spec: finalizers: - kubernetes status: phase: Active # 4 查看ns详情 命令：kubectl describe ns ns名称 [root@master ~]# kubectl describe ns default Name: default Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Status: Active # Active 命名空间正在使用中 Terminating 正在删除命名空间 # ResourceQuota 针对namespace做的资源限制 # LimitRange针对namespace中的每个组件做的资源限制 No resource quota. No LimitRange resource. 创建namespace\n1 2 3 # 创建namespace [root@master ~]# kubectl create ns dev namespace/dev created 删除\n1 2 3 # 删除namespace [root@master ~]# kubectl delete ns dev namespace \u0026#34;dev\u0026#34; deleted 配置方式\n首先准备一个yaml文件：ns-dev.yaml\n1 2 3 4 apiVersion: v1 kind: Namespace metadata: name: dev 然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f ns-dev.yaml\n删除：kubectl delete -f ns-dev.yaml\nPod Pod是kubernetes集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于Pod中。\nPod可以认为是容器的封装，一个Pod中可以存在一个或者多个容器。\nkubernetes在集群启动之后，集群中的各个组件也都是以Pod方式运行的。可以通过下面命令查看：\n创建并运行\nkubernetes没有提供单独运行Pod的命令，都是通过Pod控制器来实现的\n高版本的kubernetes创建的不是控制器，就是pod本身，可以通过kubectl create deployment 这种方式创建才有控制器，run是自由式创建，是没有控制器的，可以试试两种方式去创建nginx，用create deployment创建的nginx，用delete pod方法删了pod后会自动重启\n1 2 3 4 5 6 # 命令格式： kubectl run (pod控制器名称) [参数] # --image 指定Pod的镜像 # --port 指定端口 # --namespace 指定namespace [root@master ~]# kubectl run nginx --image=nginx:1.17.1 --port=80 --namespace dev pod/nginx created 1 2 3 kubectl create deployment nginx --image=nginx:1.17.1 --port=80 --namespace dev #页面显示deployment.apps/nginx created 查看pod信息\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 # 查看Pod基本信息 [root@master ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE nginx-5ff7956ff6-fg2db 1/1 Running 0 43s # 查看Pod的详细信息 [root@master ~]# kubectl describe pod nginx-5ff7956ff6-fg2db -n dev Name: nginx-5ff7956ff6-fg2db Namespace: dev Priority: 0 Node: node1/192.168.109.101 Start Time: Wed, 08 Apr 2020 09:29:24 +0800 Labels: pod-template-hash=5ff7956ff6 run=nginx Annotations: \u0026lt;none\u0026gt; Status: Running IP: 10.244.1.23 IPs: IP: 10.244.1.23 Controlled By: ReplicaSet/nginx-5ff7956ff6 Containers: nginx: Container ID: docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c Image: nginx:1.17.1 Image ID: docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 Port: 80/TCP Host Port: 0/TCP State: Running Started: Wed, 08 Apr 2020 09:30:01 +0800 Ready: True Restart Count: 0 Environment: \u0026lt;none\u0026gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: default-token-hwvvw: Type: Secret (a volume populated by a Secret) SecretName: default-token-hwvvw Optional: false QoS Class: BestEffort Node-Selectors: \u0026lt;none\u0026gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled \u0026lt;unknown\u0026gt; default-scheduler Successfully assigned dev/nginx-5ff7956ff6-fg2db to node1 Normal Pulling 4m11s kubelet, node1 Pulling image \u0026#34;nginx:1.17.1\u0026#34; Normal Pulled 3m36s kubelet, node1 Successfully pulled image \u0026#34;nginx:1.17.1\u0026#34; Normal Created 3m36s kubelet, node1 Created container nginx Normal Started 3m36s kubelet, node1 Started container nginx 访问Pod\n1.21此版本下 pod不能直接在宿主机访问\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # 获取podIP [root@master ~]# kubectl get pods -n dev -o wide NAME READY STATUS RESTARTS AGE IP NODE ... nginx-5ff7956ff6-fg2db 1/1 Running 0 190s 10.244.1.23 node1 ... #访问POD [root@master ~]# curl http://10.244.1.23:80 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 删除指定Pod\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 # 删除指定Pod [root@master ~]# kubectl delete pod nginx-5ff7956ff6-fg2db -n dev pod \u0026#34;nginx-5ff7956ff6-fg2db\u0026#34; deleted # 此时，显示删除Pod成功，但是再查询，发现又新产生了一个 [root@master ~]# kubectl get pods -n dev NAME READY STATUS RESTARTS AGE nginx-5ff7956ff6-jj4ng 1/1 Running 0 21s # 这是因为当前Pod是由Pod控制器创建的，控制器会监控Pod状况，一旦发现Pod死亡，会立即重建 # 此时要想删除Pod，必须删除Pod控制器 # 先来查询一下当前namespace下的Pod控制器 [root@master ~]# kubectl get deploy -n dev NAME READY UP-TO-DATE AVAILABLE AGE nginx 1/1 1 1 9m7s # 接下来，删除此PodPod控制器 [root@master ~]# kubectl delete deploy nginx -n dev deployment.apps \u0026#34;nginx\u0026#34; deleted # 稍等片刻，再查询Pod，发现Pod被删除了 [root@master ~]# kubectl get pods -n dev No resources found in dev namespace. 配置操作\n创建一个pod-nginx.yaml，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: v1 kind: Pod metadata: name: nginx namespace: dev spec: containers: - image: nginx:1.17.1 name: pod ports: - name: nginx-port containerPort: 80 protocol: TCP 然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f pod-nginx.yaml\n删除：kubectl delete -f pod-nginx.yaml\nLabel Label是kubernetes系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。\nLabel的特点：\n一个Label会以key/value键值对的形式附加到各种对象上，如Node、Pod、Service等等 一个资源对象可以定义任意数量的Label ，同一个Label也可以被添加到任意数量的资源对象上去 Label通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除 可以通过Label实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。\n一些常用的Label 示例如下：\n版本标签：\u0026ldquo;version\u0026rdquo;:\u0026ldquo;release\u0026rdquo;, \u0026ldquo;version\u0026rdquo;:\u0026ldquo;stable\u0026rdquo;\u0026hellip;\u0026hellip; 环境标签：\u0026ldquo;environment\u0026rdquo;:\u0026ldquo;dev\u0026rdquo;，\u0026ldquo;environment\u0026rdquo;:\u0026ldquo;test\u0026rdquo;，\u0026ldquo;environment\u0026rdquo;:\u0026ldquo;pro\u0026rdquo; 架构标签：\u0026ldquo;tier\u0026rdquo;:\u0026ldquo;frontend\u0026rdquo;，\u0026ldquo;tier\u0026rdquo;:\u0026ldquo;backend\u0026rdquo; 标签定义完毕之后，还要考虑到标签的选择，这就要使用到Label Selector，即：\nLabel用于给某个资源对象定义标识\nLabel Selector用于查询和筛选拥有某些标签的资源对象\n当前有两种Label Selector：\n基于等式的Label Selector name = slave: 选择所有包含Label中key=\u0026ldquo;name\u0026quot;且value=\u0026ldquo;slave\u0026quot;的对象env != production: 选择所有包括Label中的key=\u0026ldquo;env\u0026quot;且value不等于\u0026quot;production\u0026quot;的对象\n基于集合的Label Selector name in (master, slave): 选择所有包含Label中的key=\u0026ldquo;name\u0026quot;且value=\u0026ldquo;master\u0026quot;或\u0026quot;slave\u0026quot;的对象name not in (frontend): 选择所有包含Label中的key=\u0026ldquo;name\u0026quot;且value不等于\u0026quot;frontend\u0026quot;的对象\n标签的选择条件可以使用多个，此时将多个Label Selector进行组合，使用逗号\u0026rdquo;,\u0026ldquo;进行分隔即可。例如：\nname=slave，env!=production\nname not in (frontend)，env!=production\n命令方式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 # 为pod资源打标签 [root@master ~]# kubectl label pod nginx-pod version=1.0 -n dev pod/nginx-pod labeled # 为pod资源更新标签 [root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwrite pod/nginx-pod labeled # 查看标签 [root@master ~]# kubectl get pod nginx-pod -n dev --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-pod 1/1 Running 0 10m version=2.0 # 筛选标签 [root@master ~]# kubectl get pod -n dev -l version=2.0 --show-labels NAME READY STATUS RESTARTS AGE LABELS nginx-pod 1/1 Running 0 17m version=2.0 [root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labels No resources found in dev namespace. #删除标签 [root@master ~]# kubectl label pod nginx-pod version- -n dev pod/nginx-pod labeled 配置方式\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 apiVersion: v1 kind: Pod metadata: name: nginx namespace: dev labels: version: \u0026#34;3.0\u0026#34; env: \u0026#34;test\u0026#34; spec: containers: - image: nginx:1.17.1 name: pod ports: - name: nginx-port containerPort: 80 protocol: TCP 然后就可以执行对应的更新命令了：kubectl apply -f pod-nginx.yaml\nDeployment 在kubernetes中，Pod是最小的控制单元，但是kubernetes很少直接控制Pod，一般都是通过Pod控制器来完成的。Pod控制器用于pod的管理，确保pod资源符合预期的状态，当pod的资源出现故障时，会尝试进行重启或重建pod。\n在kubernetes中Pod控制器的种类有很多，本章节只介绍一种：Deployment。\nDeployment 控制器并不直接管理 Pod，而是通过管理 ReplicaSet 来间接管理 Pod ，即：Deployment 管理 ReplicaSet，ReplicaSet 管理 Pod 。所以 Deployment 的功能比 ReplicaSet 强大。\n命令操作\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 [root@master ~]# kubectl create deployment nginx --image=nginx --port=80 --replicas=3 -n dev deployment.apps/nginx created # 查看创建的deploy和Pod [root@master ~]# kubectl get deploy,pods -n dev NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/nginx 3/3 3 3 73s NAME READY STATUS RESTARTS AGE pod/nginx-7848d4b86f-dnvkv 1/1 Running 0 73s pod/nginx-7848d4b86f-fkr8j 1/1 Running 0 73s pod/nginx-7848d4b86f-pff2q 1/1 Running 0 73s # UP-TO-DATE：成功升级的副本数量 # AVAILABLE：可用副本的数量 [root@master ~]# kubectl get deploy -n dev -o wide NAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTOR nginx 3/3 3 3 2m51s nginx nginx:1.17.1 run=nginx # 查看deployment的详细信息 [root@master ~]# kubectl describe deploy nginx -n dev Name: nginx Namespace: dev CreationTimestamp: Tue, 20 Feb 2024 23:31:41 +0800 Labels: app=nginx Annotations: deployment.kubernetes.io/revision: 1 Selector: app=nginx Replicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailable StrategyType: RollingUpdate MinReadySeconds: 0 RollingUpdateStrategy: 25% max unavailable, 25% max surge Pod Template: Labels: app=nginx Containers: nginx: Image: nginx Port: 80/TCP Host Port: 0/TCP Environment: \u0026lt;none\u0026gt; Mounts: \u0026lt;none\u0026gt; Volumes: \u0026lt;none\u0026gt; Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailable OldReplicaSets: \u0026lt;none\u0026gt; NewReplicaSet: nginx-7848d4b86f (3/3 replicas created) Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 3m13s deployment-controller Scaled up replica set nginx-7848d4b86f to 3 # 删除 [root@master ~]# kubectl delete deploy nginx -n dev deployment.apps \u0026#34;nginx\u0026#34; deleted 配置操作\n创建一个deploy-nginx.yaml，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 apiVersion: apps/v1 # 版本号 kind: Deployment # 类型 metadata: # 元数据 name: nginx # rs名称 namespace: dev # 所属命名空间 labels: #标签 controller: deploy spec: # 详情描述 replicas: 3 # 副本数量 revisionHistoryLimit: 3 # 保留历史版本，默认为10 paused: false # 暂停部署，默认是false progressDeadlineSeconds: 600 # 部署超时时间（s），默认是600 strategy: # 策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: # 滚动更新 maxSurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数 maxUnavailable: 30% # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f deploy-nginx.yaml\n删除：kubectl delete -f deploy-nginx.yaml\nService 通过上节课的学习，已经能够利用Deployment来创建一组Pod来提供具有高可用性的服务。\n虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两问题：\nPod IP 会随着Pod的重建产生变化 Pod IP 仅仅是集群内可见的虚拟IP，外部无法访问 这样对于访问这个服务带来了难度。因此，kubernetes设计了Service来解决这个问题。\nService可以看作是一组同类Pod对外的访问接口。借助Service，应用可以方便地实现服务发现和负载均衡。\n操作一：创建集群内部可访问的Service\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 # 暴露Service [root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=ClusterIP --port=80 --target-port=80 -n dev service/svc-nginx1 exposed # 查看service [root@master ~]# kubectl get svc svc-nginx1 -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR svc-nginx1 ClusterIP 10.96.57.211 \u0026lt;none\u0026gt; 80/TCP 50s app=nginx-pod # 这里产生了一个CLUSTER-IP，这就是service的IP，在Service的生命周期中，这个地址是不会变动的 # 可以通过这个IP访问当前service对应的POD [root@master ~]# curl 10.109.179.231:80 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt; \u0026lt;title\u0026gt;Welcome to nginx!\u0026lt;/title\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; ....... \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 操作二：创建集群外部也可访问的Service\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 上面创建的Service的type类型为ClusterIP，这个ip地址只用集群内部可访问 # 如果需要创建外部也可以访问的Service，需要修改type为NodePort [root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=NodePort --port=80 --target-port=80 -n dev service/svc-nginx2 exposed # 此时查看，会发现出现了NodePort类型的Service，而且有一对Port（80:31928/TC） [root@master ~]# kubectl get svc svc-nginx-2 -n dev -o wide NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTOR svc-nginx2 NodePort 10.100.94.0 \u0026lt;none\u0026gt; 80:31928/TCP 9s run=nginx # 接下来就可以通过集群外的主机访问 节点IP:31928访问服务了 # 例如在的电脑主机上通过浏览器访问下面的地址 http://192.168.109.100:31928/ 删除Service\n1 2 [root@master ~]# kubectl delete svc svc-nginx1 -n dev service \u0026#34;svc-nginx1\u0026#34; deleted 配置方式\n创建一个svc-nginx.yaml，内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: v1 kind: Service metadata: name: svc-nginx namespace: dev spec: clusterIP: 10.109.179.231 ports: - port: 80 protocol: TCP targetPort: 80 selector: run: nginx type: ClusterIP 然后就可以执行对应的创建和删除命令了：\n创建：kubectl create -f svc-nginx.yaml\n删除：kubectl delete -f svc-nginx.yaml\n小结\n至此，已经掌握了Namespace、Pod、Deployment、Service资源的基本操作，有了这些操作，就可以在kubernetes集群中实现一个服务的简单部署和访问了，但是如果想要更好的使用kubernetes，就需要深入学习这几种资源的细节和原理。\n后续k8s详解可以看：\nhttps://www.yuque.com/fairy-era/yg511q/lmy7gc#9d0a1c8b\n","date":"2024-03-12T14:30:05+08:00","image":"https://xxl999227.github.io/archives/kubernetes3kubernetesv1.21%E6%A6%82%E5%BF%B5/img/image.png","permalink":"https://xxl999227.github.io/archives/kubernetes3kubernetesv1.21%E6%A6%82%E5%BF%B5/","title":"【kubernetes】3、Kubernetes（v1.21）概念"},{"content":"前置知识点 目前生产部署Kubernetes 集群主要有两种方式：\nkubeadm\nKubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。\n官方地址：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/\n二进制包\n从github 下载发行版的二进制包，手动部署每个组件，组成Kubernetes 集群。\nKubeadm 降低部署门槛，但屏蔽了很多细节，遇到问题很难排查。如果想更容易可控，推荐使用二进制包部署Kubernetes 集群，虽然手动部署麻烦点，期间可以学习很多工作原理，也利于后期维护。\nkubeadm 部署方式介绍 kubeadm 是官方社区推出的一个用于快速部署kubernetes 集群的工具，这个工具能通过两条指令完成一个kubernetes 集群的部署：\n创建一个Master 节点kubeadm init 将Node 节点加入到当前集群中$ kubeadm join \u0026lt;Master 节点的IP 和端口\u0026gt; 安装要求 在开始之前，部署Kubernetes 集群机器需要满足以下几个条件：\n一台或多台机器，操作系统CentOS7.x-86_x64，版本需要大于7.5，否则会出现问题，例如节点加入不到集群 硬件配置：2GB 或更多RAM，2 个CPU 或更多CPU，硬盘30GB 或更多 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止swap 分区 2.4 最终目标 在所有节点上安装Docker 和kubeadm 部署Kubernetes Master 部署容器网络插件 部署Kubernetes Node，将节点加入Kubernetes 集群中 部署Dashboard Web 页面，可视化查看Kubernetes 资源 准备环境 角色 IP地址 组件 master 192.168.79.100 docker，kubectl，kubeadm，kubelet node1 192.168.79.101 docker，kubectl，kubeadm，kubelet node2 192.168.79.102 docker，kubectl，kubeadm，kubelet 虚拟机准备和网络设置参考：[6-环境搭建\u0026ndash;主机安装_哔哩哔哩_bilibili]\n系统初始化 1 2 #先查看版本，必须要大于centos7.5 cat /etc/redhat-release 设置 Host 文件的相互解析（所有节点都要操作） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 cat \u0026lt;\u0026lt;EOF\u0026gt;\u0026gt; /etc/hosts 192.168.79.100 master 192.168.79.101 node1 192.168.79.102 node2 EOF #执行完后可以查看一下 vi /etc/hosts # 出现如下提示则成功 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 192.168.79.100 master 192.168.79.101 node1 192.168.79.102 node2 然后在任意一台机器上ping master或者ping node1或者ping node2都能通\n禁用 Iptables 和firewalld服务（所有节点都要操作） k8和docker运行过程中会产生大量Iptables规则，避免与系统规则混淆，先禁用\n1 2 3 4 5 6 7 #禁用 systemctl stop firewalld #关闭开机自启动 systemctl disable firewalld #本系统其实没有iptables systemctl stop iptables systemctl disable iptables 禁用 SELINUX 和swap分区（所有节点都要操作） linux的一个安全服务，如果不关闭容易遇到奇葩问题\n1 2 #setenforce 0只能暂时关闭，需要将/etc/selinux/config中的SELINUX选项改为disabled setenforce 0 \u0026amp;\u0026amp; sed -i \u0026#39;s/^SELINUX=.*/SELINUX=disabled/\u0026#39; /etc/selinux/config swap分区指的是虚拟内存分区,它的作用是在物理内存使用完之后,将磁盘空间虚拟成内存来使用\n启用swap设备会对系统的性能产生非常负面的影响,因此kubernetes要求每个节点都要禁用swap设备\n但是如果因为某些原因确实不能关闭swap分区,就需要在集群安装过程中通过明确的参数进行配置说明\n1 2 #swapoff -a只能暂时关闭，需要注释掉/etc/fstab中最后一行 swapoff -a \u0026amp;\u0026amp; sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab 修改linux内核参数（所有节点都要操作） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #加载网桥过滤模块 modprobe br_netfilter #在当前路径下生成kubernetes.conf文件，编辑内核参数，添加网桥过滤和地址转发功能， cat \u0026lt;\u0026lt;EOF\u0026gt; kubernetes.conf net.bridge.bridge-nf-call-iptables=1 net.bridge.bridge-nf-call-ip6tables=1 net.ipv4.ip_forward=1 EOF #拷贝文件到指定目录 cp kubernetes.conf /etc/sysctl.d/kubernetes.conf #重新加载配置 sysctl -p /etc/sysctl.d/kubernetes.conf 时间同步（所有节点都要操作） Kubernetes 要求集群中的节点时间必须精确一致，所以在每个节点上添加时间同步，这里直接用chronyd服务从网络同步时间 1 2 3 4 5 6 7 8 9 #启动服务 systemctl start chronyd #设置开机自启 systemctl enable chronyd #验证 date 配置ipvs（所有节点都要操作） 在 Kubernetes 中 service 有两种代理模型，一种是基于 iptables ，另一种是基于 ipvs 的。ipvs 的性能要高于 iptables 的，但是如果要使用它，需要手动载入 ipvs 模块。\n●在三台机器安装 ipset 和 ipvsadm ：\n1 2 #安装 ipset 和 ipvsadm yum -y install ipset ipvsadm 1 2 3 4 5 6 7 8 9 10 11 #生成脚本文件 cat \u0026lt;\u0026lt;EOF\u0026gt; /etc/sysconfig/modules/ipvs.modules #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF #授权脚本文件，执行，然后查看结果 chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; bash /etc/sysconfig/modules/ipvs.modules \u0026amp;\u0026amp; lsmod | grep -e ip_vs -e nf_conntrack_ipv4 重启机器 1 2 3 4 5 6 7 reboot #重启后查看配置是否生效，出现disable则ok getenforce #swap分区相关都是0则ok free -m 安装 Docker 版本20.10.8（所有节点都要操作） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #先配置阿里云的镜像源 yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #查看存储库中 Docker 的版本 yum list docker-ce --showduplicates | sort -r #安装指定版本的 Docker（v20.10） yum -y install docker-ce-3:20.10.8-3.el7.x86_64 docker-ce-cli-1:20.10.8-3.el7.x86_64 containerd.io ## 创建 /etc/docker 目录 mkdir /etc/docker cat \u0026gt; /etc/docker/daemon.json \u0026lt;\u0026lt;EOF { \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://7u7aivp6.mirror.aliyuncs.com\u0026#34;] } EOF # 由于没有启动过docker，所以无需重载配置文件 systemctl daemon-reload #启动docker并设置开机自启动 systemctl start docker \u0026amp;\u0026amp; systemctl enable docker 安装 kubelet kubeadm kubectl 指定版本1.21.10 （所有节点都要操作） 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 #配置kubernetes镜像源 cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF #安装kubelet kubeadm kubectl 指定版本 yum install -y kubelet-1.21.10 kubeadm-1.21.10 kubectl-1.21.10 #为了实现 Docker 使用的 cgroup drvier 和 kubelet 使用的 cgroup drver 一致， #建议修改 /etc/sysconfig/kubelet 文件的内容： vim /etc/sysconfig/kubelet # 修改 KUBELET_EXTRA_ARGS=\u0026#34;--cgroup-driver=systemd\u0026#34; KUBE_PROXY_MODE=\u0026#34;ipvs\u0026#34; #设置开机自启，由于没有生成配置文件，集群初始化后自动启动： systemctl enable kubelet 下载 Kubernetes 所需镜像 查看 Kubernetes 安装所需镜像：\n1 kubeadm config images list :::color2 从阿里云拉取所需的七个镜像\n:::\n1 2 3 4 5 6 7 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.21.10 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.21.10 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.21.10 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.21.10 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.4.1 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.13-0 docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #查看镜像，这时可以看到镜像的tag不对，需要改为kubernetes的tag，方便后续使用 docker images #给镜像重新打tag docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-apiserver:v1.21.10 k8s.gcr.io/kube-apiserver:v1.21.10 \u0026amp;\u0026amp; docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager:v1.21.10 k8s.gcr.io/kube-controller-manager:v1.21.10 \u0026amp;\u0026amp; docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-scheduler:v1.21.10 k8s.gcr.io/kube-scheduler:v1.21.10 \u0026amp;\u0026amp; docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy:v1.21.10 k8s.gcr.io/kube-proxy:v1.21.10 \u0026amp;\u0026amp; docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.4.1 k8s.gcr.io/pause:3.4.1 \u0026amp;\u0026amp; docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/etcd:3.4.13-0 k8s.gcr.io/etcd:3.4.13-0 \u0026amp;\u0026amp; docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/coredns:v1.8.0 k8s.gcr.io/coredns:v1.8.0 #可以移除老镜像，也可以不移除 部署Kubernetes Master 初始化主节点（主节点操作） 注意：\napiserver-advertise-address 一定要是master主机的 IP 地址。当然，你可以在任意一台机器上执行下面的命令，它就是主节点 apiserver-advertise-address 、service-cidr 和 pod-network-cidr 不能在同一个网络范围内。 不要使用 172.17.0.1/16 网段范围，因为这是 Docker 默认使用的。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 #第一个参数为master节点的地址，由于默认拉取镜像地址k8s.gcr.io国内无法访问，这里需要指定阿里云镜像仓库地址 kubeadm init \\ --apiserver-advertise-address=192.168.79.100 \\ --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers \\ --kubernetes-version=v1.21.10 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=10.244.0.0/16 #安装成功后可以看到 Your Kubernetes control-plane has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Alternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.conf You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ Then you can join any number of worker nodes by running the following on each as root: kubeadm join 192.168.79.100:6443 --token kpoksa.a5dhuqu4ccbap85o \\ --discovery-token-ca-cert-hash sha256:e070c812307c6aaaccb328790001ce5bad043a994106b0949c4930b81e93eadc #然后按照指示执行如下操作 mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config # 如果是 root 用户，还可以执行如下命令 export KUBECONFIG=/etc/kubernetes/admin.conf #此时如果执行kubectl get nodes，可以看到没有任何工作节点 kubectl get nodes 默认的 token 有效期为 24 小时，当过期之后，该 token 就不能用了，这时可以使用如下的命令创建 token ：\n1 kubeadm token create --print-join-command 生成一个永不过期的token\n1 kubeadm token create --ttl 0 --print-join-command 其余工作节点加入主节点 按照上一步的指示，将其余的工作节点加入到主节点\n1 2 kubeadm join 192.168.79.100:6443 --token kpoksa.a5dhuqu4ccbap85o \\ --discovery-token-ca-cert-hash sha256:e070c812307c6aaaccb328790001ce5bad043a994106b0949c4930b81e93eadc 然后再在master节点运行kubectl get nodes可以看到相应的工作节点了\n部署网络 官网 Kubernetes 支持多种网络插件，比如 flannel、calico、canal 等，任选一种即可，本次选择 calico（在 192.168.65.100 节点上执行，网络不行，请点这里 calico.yaml）。 calico 和 k8s 的版本对应 1 2 3 4 5 6 kubectl apply -f https://projectcalico.docs.tigera.io/v3.19/manifests/calico.yaml #网络不好将calico.yaml文件上传到root目录下然后执行 kubectl apply -f /root/calico.yaml #网络这一步的等待时间可能比较久，中间可能会出现部分节点就绪，部分未就绪的情况 然后查看节点状态\n1 2 3 4 5 6 7 8 kubectl get nodes #结果如下 [root@master /]# kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready control-plane,master 27m v1.21.10 node1 Ready \u0026lt;none\u0026gt; 21m v1.21.10 node2 Ready \u0026lt;none\u0026gt; 21m v1.21.10 至此，Kubernetes安装完成 让工作节点也能使用kubectl kubectl的运行是需要进行配置的,它的配置文件是$HOME/.kube,如果想要在node节点运行此命令,需要将\nmaster上的.kube文件复制到node节点上,即在master节点上执行下面操作:\n1 scp -r $HOME/.kube node1:$HOME/ \u0026amp;\u0026amp; scp -r $HOME/.kube node2:$HOME/ 测试kubernetes 集群 部署nginx 测试 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #在控制节点执行安装nginx kubectl create deployment nginx --image=nginx:1.14-alpine #暴露80端口 kubectl expose deployment nginx --port=80 --type=NodePort #查看pod和service kubectl get pod,svc #结果如下 NAME READY STATUS RESTARTS AGE pod/nginx-65c4bffcb6-knqpr 1/1 Running 0 2m20s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 37m service/nginx NodePort 10.96.129.185 \u0026lt;none\u0026gt; 80:32100/TCP 100s 此时可以使用master节点的ip加nginx的端口访问，例如 192.168.79.100:32100\n","date":"2024-02-04T18:19:44+08:00","permalink":"https://xxl999227.github.io/archives/kubernetes2%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BAk8skubeadm%E6%96%B9%E5%BC%8F/","title":"【kubernetes】2、集群环境搭建k8s(kubeadm方式)"},{"content":"应用部署方式演变 在部署应用程序的方式上，主要经历了三个时代：\n传统部署：互联网早期，会直接将应用程序部署在物理机上 优点：简单，不需要其它技术的参与\n缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响\n虚拟化部署：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境 优点：程序环境不会相互产生影响，提供了一定程度的安全性\n缺点：增加了操作系统，浪费了部分资源\n容器化部署：与虚拟化类似，但是共享了操作系统 优点：可以保证每个容器拥有自己的文件系统、CPU、内存、进程空间等\n运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦\n容器化的应用程序可以跨云服务商、跨Linux操作系统发行版进行部署\n容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说：\n一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器 当并发访问量变大的时候，怎么样做到横向扩展容器数量 这些容器管理的问题统称为容器编排问题，为了解决这些容器编排问题，就产生了一些容器编排的软件：\nSwarm：Docker自己的容器编排工具 Mesos：Apache的一个资源统一管控的工具，需要和Marathon结合使用 Kubernetes：Google开源的的容器编排工具 kubernetes简介 kubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器\u0026mdash;-Borg系统的一个开源版本，于2014年9月发布第一个版本，2015年7月发布第一个正式版本。\nkubernetes的本质是一组服务器集群，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能：\n自我修复：一旦某一个容器崩溃，能够在1秒中左右迅速启动新的容器 弹性伸缩：可以根据需要，自动对集群中正在运行的容器数量进行调整 服务发现：服务可以通过自动发现的形式找到它所依赖的服务 负载均衡：如果一个服务起动了多个容器，能够自动实现请求的负载均衡 版本回退：如果发现新发布的程序版本有问题，可以立即回退到原来的版本 存储编排：可以根据容器自身的需求自动创建存储卷 kubernetes组件 一个kubernetes集群主要是由控制节点(master)、工作节点(node)构成，每个节点上都会安装不同的组件。\nmaster：集群的控制平面，负责集群的决策 ( 管理 )\nApiServer : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制\nScheduler : 负责集群资源调度，按照预定的调度策略将Pod调度到相应的node节点上\nControllerManager : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等\n**Etcd**** \u0026lt;/fo：负责存储集群中各种资源对象的信息\nnode：集群的数据平面，负责为容器提供运行环境 ( 干活 )\nKubelet : 负责维护容器的生命周期，即通过控制docker，来创建、更新、销毁容器\nKubeProxy : 负责提供集群内部的服务发现和负载均衡\nDocker : 负责节点上容器的各种操作\n下面，以部署一个nginx服务来说明kubernetes系统各个组件调用关系：\n首先要明确，一旦kubernetes环境启动之后，master和node都会将自身的信息存储到etcd数据库中 一个nginx服务的安装请求会首先被发送到master节点的apiServer组件 apiServer组件会调用scheduler组件来决定到底应该把这个服务安装到哪个node节点上在此时，它会从etcd中读取各个node节点的信息，然后按照一定的算法进行选择，并将结果告知apiServer apiServer调用controller-manager去调度Node节点安装nginx服务 kubelet接收到指令后，会通知docker，然后由docker来启动一个nginx的podpod是kubernetes的最小操作单元，容器必须跑在pod中至此， 一个nginx服务就运行了，如果需要访问nginx，就需要通过kube-proxy来对pod产生访问的代理 这样，外界用户就可以访问集群中的nginx服务了\nkubernetes概念 Master：集群控制节点，每个集群需要至少一个master节点负责集群的管控\nNode：工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行\nPod：kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器\nController：控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等\nService：pod对外服务的统一入口，下面可以维护者同一类的多个pod\nLabel：标签，用于对pod进行分类，同一类pod会拥有相同的标签\nNameSpace：命名空间，用来隔离pod的运行环境\n","date":"2024-01-28T22:27:57+08:00","image":"https://xxl999227.github.io/archives/kubernetes1kubernetes%E4%BB%8B%E7%BB%8D/img/image.png","permalink":"https://xxl999227.github.io/archives/kubernetes1kubernetes%E4%BB%8B%E7%BB%8D/","title":"【kubernetes】1、kubernetes介绍"},{"content":"好了，我们已经熟悉了Docker的基本用法，接下来可以尝试部署项目了。\n在课前资料中已经提供了一个黑马商城项目给大家，如图：\n项目说明：\nhmall：商城的后端代码 hmall-portal：商城用户端的前端代码 hmall-admin：商城管理端的前端代码 部署的容器及端口说明：\n项目 容器名 端口 备注 hmall hmall 8080 黑马商城后端API入口 hmall-portal nginx 18080 黑马商城用户端入口 hmall-admin 18081 黑马商城管理端入口 mysql mysql 3306 数据库 在正式部署前，我们先删除之前的nginx、dd两个容器：\n1 docker rm -f nginx dd mysql容器中已经准备好了商城的数据，所以就不再删除了。\n部署Java项目 hmall项目是一个maven聚合项目，使用IDEA打开hmall项目，查看项目结构如图：\n我们要部署的就是其中的hm-service，其中的配置文件采用了多环境的方式：\n其中的application-dev.yaml是部署到开发环境的配置，application-local.yaml是本地运行时的配置。\n查看application.yaml，你会发现其中的JDBC地址并未写死，而是读取变量：\n这两个变量在application-dev.yaml和application-local.yaml中并不相同：\n在dev开发环境（也就是Docker部署时）采用了mysql作为地址，刚好是我们的mysql容器名，只要两者在一个网络，就一定能互相访问。\n我们将项目打包：\n结果：\n将hm-service目录下的Dockerfile和hm-service/target目录下的hm-service.jar一起上传到虚拟机的root目录：\n部署项目：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 1.构建项目镜像，不指定tag，则默认为latest docker build -t hmall . # 2.查看镜像 docker images # 结果 REPOSITORY TAG IMAGE ID CREATED SIZE hmall latest 0bb07b2c34b9 43 seconds ago 362MB docker-demo 1.0 49743484da68 24 hours ago 327MB nginx latest 605c77e624dd 16 months ago 141MB mysql latest 3218b38490ce 17 months ago 516MB # 3.创建并运行容器，并通过--network将其加入hmall网络，这样才能通过容器名访问mysql docker run -d --name hmall --network hmall -p 8080:8080 hmall 测试，通过浏览器访问：http://你的虚拟机地址:8080/search/list\n部署前端 hmall-portal和hmall-admin是前端代码，需要基于nginx部署。在课前资料中已经给大家提供了nginx的部署目录：\n其中：\nhtml是静态资源目录，我们需要把hmall-portal以及hmall-admin都复制进去 nginx.conf是nginx的配置文件，主要是完成对html下的两个静态资源目录做代理 我们现在要做的就是把整个nginx目录上传到虚拟机的/root目录下：\n然后创建nginx容器并完成两个挂载：\n把/root/nginx/nginx.conf挂载到/etc/nginx/nginx.conf 把/root/nginx/html挂载到/usr/share/nginx/html 由于需要让nginx同时代理hmall-portal和hmall-admin两套前端资源，因此我们需要暴露两个端口：\n18080：对应hmall-portal 18081：对应hmall-admin 命令如下：\n1 2 3 4 5 6 7 8 docker run -d \\ --name nginx \\ -p 18080:18080 \\ -p 18081:18081 \\ -v /root/nginx/html:/usr/share/nginx/html \\ -v /root/nginx/nginx.conf:/etc/nginx/nginx.conf \\ --network hmall \\ nginx 测试，通过浏览器访问：http://你的虚拟机ip:18080\nDockerCompose 大家可以看到，我们部署一个简单的java项目，其中包含3个容器：\nMySQL Nginx Java项目 而稍微复杂的项目，其中还会有各种各样的其它中间件，需要部署的东西远不止3个。如果还像之前那样手动的逐一部署，就太麻烦了。\n而Docker Compose就可以帮助我们实现多个相互关联的Docker容器的快速部署。它允许用户通过一个单独的 docker-compose.yml 模板文件（YAML 格式）来定义一组相关联的应用容器。\n基本语法 docker-compose.yml文件的基本语法可以参考官方文档：\nhttps://docs.docker.com/compose/compose-file/compose-file-v3/\ndocker-compose文件中可以定义多个相互关联的应用容器，每一个应用容器被称为一个服务（service）。由于service就是在定义某个应用的运行时参数，因此与docker run参数非常相似。\n举例来说，用docker run部署MySQL的命令如下：\n1 2 3 4 5 6 7 8 9 10 docker run -d \\ --name mysql \\ -p 3306:3306 \\ -e TZ=Asia/Shanghai \\ -e MYSQL_ROOT_PASSWORD=123 \\ -v ./mysql/data:/var/lib/mysql \\ -v ./mysql/conf:/etc/mysql/conf.d \\ -v ./mysql/init:/docker-entrypoint-initdb.d \\ --network hmall mysql 如果用docker-compose.yml文件来定义，就是这样：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 version: \u0026#34;3.8\u0026#34; services: mysql: image: mysql container_name: mysql ports: - \u0026#34;3306:3306\u0026#34; environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 123 volumes: - \u0026#34;./mysql/conf:/etc/mysql/conf.d\u0026#34; - \u0026#34;./mysql/data:/var/lib/mysql\u0026#34; networks: - new networks: new: name: hmall 对比如下：\ndocker run 参数 docker compose 指令 说明 \u0026ndash;name container_name 容器名称 -p ports 端口映射 -e environment 环境变量 -v volumes 数据卷配置 \u0026ndash;network networks 网络 明白了其中的对应关系，相信编写docker-compose文件应该难不倒大家。\n黑马商城部署文件：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 version: \u0026#34;3.8\u0026#34; services: mysql: image: mysql container_name: mysql ports: - \u0026#34;3306:3306\u0026#34; environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 123 volumes: - \u0026#34;./mysql/conf:/etc/mysql/conf.d\u0026#34; - \u0026#34;./mysql/data:/var/lib/mysql\u0026#34; - \u0026#34;./mysql/init:/docker-entrypoint-initdb.d\u0026#34; networks: - hm-net hmall: build: context: . dockerfile: Dockerfile container_name: hmall ports: - \u0026#34;8080:8080\u0026#34; networks: - hm-net depends_on: - mysql nginx: image: nginx container_name: nginx ports: - \u0026#34;18080:18080\u0026#34; - \u0026#34;18081:18081\u0026#34; volumes: - \u0026#34;./nginx/nginx.conf:/etc/nginx/nginx.conf\u0026#34; - \u0026#34;./nginx/html:/usr/share/nginx/html\u0026#34; depends_on: - hmall networks: - hm-net networks: hm-net: name: hmall 基础命令 编写好docker-compose.yml文件，就可以部署项目了。常见的命令：\nhttps://docs.docker.com/compose/reference/\n基本语法如下：\n1 docker compose [OPTIONS] [COMMAND] 其中，OPTIONS和COMMAND都是可选参数，比较常见的有：\n类型 参数或指令 说明 Options -f 指定compose文件的路径和名称 -p 指定project名称。project就是当前compose文件中设置的多个service的集合，是逻辑概念 Commands up 创建并启动所有service容器 down 停止并移除所有容器、网络 ps 列出所有启动的容器 logs 查看指定容器的日志 stop 停止容器 start 启动容器 restart 重启容器 top 查看运行的进程 exec 在指定的运行中容器中执行命令 教学演示：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 # 1.进入root目录 cd /root # 2.删除旧容器 docker rm -f $(docker ps -qa) # 3.删除hmall镜像 docker rmi hmall # 4.清空MySQL数据 rm -rf mysql/data # 5.启动所有, -d 参数是后台启动 docker compose up -d # 结果： [+] Building 15.5s (8/8) FINISHED =\u0026gt; [internal] load build definition from Dockerfile 0.0s =\u0026gt; =\u0026gt; transferring dockerfile: 358B 0.0s =\u0026gt; [internal] load .dockerignore 0.0s =\u0026gt; =\u0026gt; transferring context: 2B 0.0s =\u0026gt; [internal] load metadata for docker.io/library/openjdk:11.0-jre-buster 15.4s =\u0026gt; [1/3] FROM docker.io/library/openjdk:11.0-jre-buster@sha256:3546a17e6fb4ff4fa681c3 0.0s =\u0026gt; [internal] load build context 0.0s =\u0026gt; =\u0026gt; transferring context: 98B 0.0s =\u0026gt; CACHED [2/3] RUN ln -snf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026amp;\u0026amp; echo 0.0s =\u0026gt; CACHED [3/3] COPY hm-service.jar /app.jar 0.0s =\u0026gt; exporting to image 0.0s =\u0026gt; =\u0026gt; exporting layers 0.0s =\u0026gt; =\u0026gt; writing image sha256:32eebee16acde22550232f2eb80c69d2ce813ed099640e4cfed2193f71 0.0s =\u0026gt; =\u0026gt; naming to docker.io/library/root-hmall 0.0s [+] Running 4/4 ✔ Network hmall Created 0.2s ✔ Container mysql Started 0.5s ✔ Container hmall Started 0.9s ✔ Container nginx Started 1.5s # 6.查看镜像 docker compose images # 结果 CONTAINER REPOSITORY TAG IMAGE ID SIZE hmall root-hmall latest 32eebee16acd 362MB mysql mysql latest 3218b38490ce 516MB nginx nginx latest 605c77e624dd 141MB # 7.查看容器 docker compose ps # 结果 NAME IMAGE COMMAND SERVICE CREATED STATUS PORTS hmall root-hmall \u0026#34;java -jar /app.jar\u0026#34; hmall 54 seconds ago Up 52 seconds 0.0.0.0:8080-\u0026gt;8080/tcp, :::8080-\u0026gt;8080/tcp mysql mysql \u0026#34;docker-entrypoint.s…\u0026#34; mysql 54 seconds ago Up 53 seconds 0.0.0.0:3306-\u0026gt;3306/tcp, :::3306-\u0026gt;3306/tcp, 33060/tcp nginx nginx \u0026#34;/docker-entrypoint.…\u0026#34; nginx 54 seconds ago Up 52 seconds 80/tcp, 0.0.0.0:18080-18081-\u0026gt;18080-18081/tcp, :::18080-18081-\u0026gt;18080-18081/tcp 打开浏览器，访问：http://yourIp:18080\n","date":"2024-01-22T23:50:51+08:00","image":"https://xxl999227.github.io/archives/docker%E7%AF%874%E9%83%A8%E7%BD%B2java%E9%A1%B9%E7%9B%AE/img/image.png","permalink":"https://xxl999227.github.io/archives/docker%E7%AF%874%E9%83%A8%E7%BD%B2java%E9%A1%B9%E7%9B%AE/","title":"【docker篇】4、部署java项目"},{"content":"接下来，我们一起来学习Docker使用的一些基础知识，为将来部署项目打下基础。具体用法可以参考Docker官方文档：\nhttps://docs.docker.com/\n常见命令 首先我们来学习Docker中的常见命令，可以参考官方文档：\nhttps://docs.docker.com/engine/reference/commandline/cli/\n命令介绍 其中，比较常见的命令有：\n命令 说明 文档地址 docker pull 拉取镜像 docker pull docker push 推送镜像到DockerRegistry docker push docker images 查看本地镜像 docker images docker rmi 删除本地镜像 docker rmi docker run 创建并运行容器（不能重复创建） docker run docker stop 停止指定容器 docker stop docker start 启动指定容器 docker start docker restart 重新启动容器 docker restart docker rm 删除指定容器 docs.docker.com docker ps 查看容器 docker ps docker logs 查看容器运行日志 docker logs docker exec 进入容器 docker exec docker save 保存镜像到本地压缩文件 docker save docker load 加载本地压缩文件到镜像 docker load docker inspect 查看容器详细信息 docker inspect 用一副图来表示这些命令的关系：\n补充：\n默认情况下，每次重启虚拟机我们都需要手动启动Docker和Docker中的容器。通过命令可以实现开机自启：\n1 2 3 4 5 # Docker开机自启 systemctl enable docker # Docker容器开机自启 docker update --restart=always [容器名/容器id] 演示 教学环节说明：我们以Nginx为例给大家演示上述命令。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 # 第1步，去DockerHub查看nginx镜像仓库及相关信息 # 第2步，拉取Nginx镜像 docker pull nginx # 第3步，查看镜像 docker images # 结果如下： REPOSITORY TAG IMAGE ID CREATED SIZE nginx latest 605c77e624dd 16 months ago 141MB mysql latest 3218b38490ce 17 months ago 516MB # 第4步，创建并允许Nginx容器 docker run -d --name nginx -p 80:80 nginx # 第5步，查看运行中容器 docker ps # 也可以加格式化方式访问，格式会更加清爽 docker ps --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Ports}}\\t{{.Status}}\\t{{.Names}}\u0026#34; # 第6步，访问网页，地址：http://虚拟机地址 # 第7步，停止容器 docker stop nginx # 第8步，查看所有容器 docker ps -a --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Ports}}\\t{{.Status}}\\t{{.Names}}\u0026#34; # 第9步，再次启动nginx容器 docker start nginx # 第10步，再次查看容器 docker ps --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Ports}}\\t{{.Status}}\\t{{.Names}}\u0026#34; # 第11步，查看容器详细信息 docker inspect nginx # 第12步，进入容器,查看容器内目录 docker exec -it nginx bash # 或者，可以进入MySQL docker exec -it mysql mysql -uroot -p # 第13步，删除容器 docker rm nginx # 发现无法删除，因为容器运行中，强制删除容器 docker rm -f nginx 命令别名 给常用Docker命令起别名，方便我们访问使用：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 修改/root/.bashrc文件 vi /root/.bashrc 内容如下： # .bashrc # User specific aliases and functions alias rm=\u0026#39;rm -i\u0026#39; alias cp=\u0026#39;cp -i\u0026#39; alias mv=\u0026#39;mv -i\u0026#39; alias dps=\u0026#39;docker ps --format \u0026#34;table {{.ID}}\\t{{.Image}}\\t{{.Ports}}\\t{{.Status}}\\t{{.Names}}\u0026#34;\u0026#39; alias dis=\u0026#39;docker images\u0026#39; # Source global definitions if [ -f /etc/bashrc ]; then . /etc/bashrc fi 然后，执行命令使别名生效\n1 source /root/.bashrc 接下来，试试看新的命令吧。\n数据卷 容器是隔离环境，容器内程序的文件、配置、运行时产生的容器都在容器内部，我们要读写容器内的文件非常不方便。大家思考几个问题：\n如果要升级MySQL版本，需要销毁旧容器，那么数据岂不是跟着被销毁了？ MySQL、Nginx容器运行后，如果我要修改其中的某些配置该怎么办？ 我想要让Nginx代理我的静态资源怎么办？ 因此，容器提供程序的运行环境，但是程序运行产生的数据、程序运行依赖的配置都应该与容器****解耦。\n什么是数据卷 数据卷（volume）是一个虚拟目录，是容器内目录与宿主机****目录之间映射的桥梁。\n以Nginx为例，我们知道Nginx中有两个关键的目录：\nhtml：放置一些静态资源 conf：放置配置文件 如果我们要让Nginx代理我们的静态资源，最好是放到html目录；如果我们要修改Nginx的配置，最好是找到conf下的nginx.conf文件。\n但遗憾的是，容器运行的Nginx所有的文件都在容器内部。所以我们必须利用数据卷将两个目录与宿主机目录关联，方便我们操作。如图：\n在上图中：\n我们创建了两个数据卷：conf、html Nginx容器内部的conf目录和html目录分别与两个数据卷关联。 而数据卷conf和html分别指向了宿主机的/var/lib/docker/volumes/conf/_data目录和/var/lib/docker/volumes/html/_data目录 这样以来，容器内的conf和html目录就 与宿主机的conf和html目录关联起来，我们称为挂载。此时，我们操作宿主机的/var/lib/docker/volumes/html/_data就是在操作容器内的/usr/share/nginx/html/_data目录。只要我们将静态资源放入宿主机对应目录，就可以被Nginx代理了。\n小提示：\n/var/lib/docker/volumes这个目录就是默认的存放所有容器数据卷的目录，其下再根据数据卷名称创建新目录，格式为/数据卷名/_data。\n为什么不让容器目录直接指向宿主机目录呢？\n因为直接指向宿主机目录就与宿主机强耦合了，如果切换了环境，宿主机目录就可能发生改变了。由于容器一旦创建，目录挂载就无法修改，这样容器就无法正常工作了。 但是容器指向数据卷，一个逻辑名称，而数据卷再指向宿主机目录，就不存在强耦合。如果宿主机目录发生改变，只要改变数据卷与宿主机目录之间的映射关系即可。 不过，我们通过由于数据卷目录比较深，不好寻找，通常我们也允许让容器直接与宿主机目录挂载而不使用数据卷，具体参考2.2.3小节。\n数据卷命令 数据卷的相关命令有：\n命令 说明 文档地址 docker volume create 创建数据卷 docker volume create docker volume ls 查看所有数据卷 docs.docker.com docker volume rm 删除指定数据卷 docs.docker.com docker volume inspect 查看某个数据卷的详情 docs.docker.com docker volume prune 清除数据卷 docker volume prune 注意：容器与数据卷的挂载要在创建容器时配置，对于创建好的容器，是不能设置数据卷的。而且创建容器的过程中，数据卷会自动创建。\n教学演示环节：演示一下nginx的html目录挂载\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 # 1.首先创建容器并指定数据卷，注意通过 -v 参数来指定数据卷 docker run -d --name nginx -p 80:80 -v html:/usr/share/nginx/html nginx # 2.然后查看数据卷 docker volume ls # 结果 DRIVER VOLUME NAME local 29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f local html # 3.查看数据卷详情 docker volume inspect html # 结果 [ { \u0026#34;CreatedAt\u0026#34;: \u0026#34;2024-05-17T19:57:08+08:00\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, \u0026#34;Labels\u0026#34;: null, \u0026#34;Mountpoint\u0026#34;: \u0026#34;/var/lib/docker/volumes/html/_data\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;html\u0026#34;, \u0026#34;Options\u0026#34;: null, \u0026#34;Scope\u0026#34;: \u0026#34;local\u0026#34; } ] # 4.查看/var/lib/docker/volumes/html/_data目录 ll /var/lib/docker/volumes/html/_data # 可以看到与nginx的html目录内容一样，结果如下： 总用量 8 -rw-r--r--. 1 root root 497 12月 28 2021 50x.html -rw-r--r--. 1 root root 615 12月 28 2021 index.html # 5.进入该目录，并随意修改index.html内容 cd /var/lib/docker/volumes/html/_data vi index.html # 6.打开页面，查看效果 # 7.进入容器内部，查看/usr/share/nginx/html目录内的文件是否变化 docker exec -it nginx bash 教学演示环节：演示一下MySQL的匿名数据卷\n1 2 3 # 1.查看MySQL容器详细信息 docker inspect mysql # 关注其中.Config.Volumes部分和.Mounts部分 我们关注两部分内容，第一是.Config.Volumes部分：\n1 2 3 4 5 6 7 8 9 { \u0026#34;Config\u0026#34;: { // ... 略 \u0026#34;Volumes\u0026#34;: { \u0026#34;/var/lib/mysql\u0026#34;: {} } // ... 略 } } 可以发现这个容器声明了一个本地目录，需要挂载数据卷，但是数据卷未定义。这就是匿名卷。\n然后，我们再看结果中的.Mounts部分：\n1 2 3 4 5 6 7 8 9 10 11 { \u0026#34;Mounts\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;volume\u0026#34;, \u0026#34;Name\u0026#34;: \u0026#34;29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/var/lib/docker/volumes/29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f/_data\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/var/lib/mysql\u0026#34;, \u0026#34;Driver\u0026#34;: \u0026#34;local\u0026#34;, } ] } 可以发现，其中有几个关键属性：\nName：数据卷名称。由于定义容器未设置容器名，这里的就是匿名卷自动生成的名字，一串hash值。 Source：宿主机目录 Destination : 容器内的目录 上述配置是将容器内的/var/lib/mysql这个目录，与数据卷29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f挂载。于是在宿主机中就有了/var/lib/docker/volumes/29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f/_data这个目录。这就是匿名数据卷对应的目录，其使用方式与普通数据卷没有差别。\n接下来，可以查看该目录下的MySQL的data文件：\n1 ls -l /var/lib/docker/volumes/29524ff09715d3688eae3f99803a2796558dbd00ca584a25a4bbc193ca82459f/_data 注意：每一个不同的镜像，将来创建容器后内部有哪些目录可以挂载，可以参考DockerHub对应的页面\n挂载本地目录或文件 可以发现，数据卷的目录结构较深，如果我们去操作数据卷目录会不太方便。在很多情况下，我们会直接将容器目录与宿主机指定目录挂载。挂载语法与数据卷类似：\n1 2 3 4 # 挂载本地目录 -v 本地目录:容器内目录 # 挂载本地文件 -v 本地文件:容器内文件 注意：本地目录或文件必须以 / 或 ./开头，如果直接以名字开头，会被识别为数据卷名而非本地目录名。\n例如：\n1 2 -v mysql:/var/lib/mysql # 会被识别为一个数据卷叫mysql，运行时会自动创建这个数据卷 -v ./mysql:/var/lib/mysql # 会被识别为当前目录下的mysql目录，运行时如果不存在会创建目录 教学演示，删除并重新创建mysql容器，并完成本地目录挂载：\n挂载/root/mysql/data到容器内的/var/lib/mysql目录 挂载/root/mysql/init到容器内的/docker-entrypoint-initdb.d目录（初始化的SQL脚本目录） 挂载/root/mysql/conf到容器内的/etc/mysql/conf.d目录（这个是MySQL配置文件目录） 在课前资料中已经准备好了mysql的init目录和conf目录：\n以及对应的初始化SQL脚本和配置文件：\n其中，hm.cnf主要是配置了MySQL的默认编码，改为utf8mb4；而hmall.sql则是后面我们要用到的黑马商城项目的初始化SQL脚本。\n我们直接将整个mysql目录上传至虚拟机的/root目录下：\n接下来，我们演示本地目录挂载：\n其中 / 表示绝对路径，./ 表示相对路径当前目录下\n-v ./mysql/data:/var/lib/mysql \\ 表示将mysql容器的data挂载到宿主机的mysql/data下 -v ./mysql/conf:/etc/mysql/conf.d \\ 表示将mysql容器的配置文件挂载到宿主机的mysql/conf下 -v ./mysql/init:/docker-entrypoint-initdb.d \\ 表示将mysql容器的初始化文件挂载到宿主机的mysql/init下，初始化的sql脚本会在第一次启动容器时自动运行，并且新增的数据也不会随着容器被删除而消失，下一次可以将mysql挂载到同一个地方，数据会仍然存在 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 # 1.删除原来的MySQL容器 docker rm -f mysql # 2.进入root目录 cd ~ # 3.创建并运行新mysql容器，挂载本地目录 docker run -d \\ --name mysql \\ -p 3306:3306 \\ -e TZ=Asia/Shanghai \\ -e MYSQL_ROOT_PASSWORD=123456 \\ -v ./mysql/data:/var/lib/mysql \\ -v ./mysql/conf:/etc/mysql/conf.d \\ -v ./mysql/init:/docker-entrypoint-initdb.d \\ mysql # 4.查看root目录，可以发现~/mysql/data目录已经自动创建好了 ls -l mysql # 结果： 总用量 4 drwxr-xr-x. 2 root root 20 5月 19 15:11 conf drwxr-xr-x. 7 polkitd root 4096 5月 19 15:11 data drwxr-xr-x. 2 root root 23 5月 19 15:11 init # 查看data目录，会发现里面有大量数据库数据，说明数据库完成了初始化 ls -l data # 5.查看MySQL容器内数据 # 5.1.进入MySQL docker exec -it mysql mysql -uroot -p123456 # 5.2.查看编码表 show variables like \u0026#34;%char%\u0026#34;; # 5.3.结果，发现编码是utf8mb4没有问题 +--------------------------+--------------------------------+ | Variable_name | Value | +--------------------------+--------------------------------+ | character_set_client | utf8mb4 | | character_set_connection | utf8mb4 | | character_set_database | utf8mb4 | | character_set_filesystem | binary | | character_set_results | utf8mb4 | | character_set_server | utf8mb4 | | character_set_system | utf8mb3 | | character_sets_dir | /usr/share/mysql-8.0/charsets/ | +--------------------------+--------------------------------+ # 6.查看数据 # 6.1.查看数据库 show databases; # 结果，hmall是黑马商城数据库 +--------------------+ | Database | +--------------------+ | hmall | | information_schema | | mysql | | performance_schema | | sys | +--------------------+ 5 rows in set (0.00 sec) # 6.2.切换到hmall数据库 use hmall; # 6.3.查看表 show tables; # 结果： +-----------------+ | Tables_in_hmall | +-----------------+ | address | | cart | | item | | order | | order_detail | | order_logistics | | pay_order | | user | +-----------------+ # 6.4.查看address表数据 +----+---------+----------+--------+----------+-------------+---------------+-----------+------------+-------+ | id | user_id | province | city | town | mobile | street | contact | is_default | notes | +----+---------+----------+--------+----------+-------------+---------------+-----------+------------+-------+ | 59 | 1 | 北京 | 北京 | 朝阳区 | 13900112222 | 金燕龙办公楼 | 李佳诚 | 0 | NULL | | 60 | 1 | 北京 | 北京 | 朝阳区 | 13700221122 | 修正大厦 | 李佳红 | 0 | NULL | | 61 | 1 | 上海 | 上海 | 浦东新区 | 13301212233 | 航头镇航头路 | 李佳星 | 1 | NULL | | 63 | 1 | 广东 | 佛山 | 永春 | 13301212233 | 永春武馆 | 李晓龙 | 0 | NULL | +----+---------+----------+--------+----------+-------------+---------------+-----------+------------+-------+ 4 rows in set (0.00 sec) 镜像 前面我们一直在使用别人准备好的镜像，那如果我要部署一个Java项目，把它打包为一个镜像该怎么做呢？\n镜像结构 要想自己构建镜像，必须先了解镜像的结构。\n之前我们说过，镜像之所以能让我们快速跨操作系统部署应用而忽略其运行环境、配置，就是因为镜像中包含了程序运行需要的系统函数库、环境、配置、依赖。\n因此，自定义镜像本质就是依次准备好程序运行的基础环境、依赖、应用本身、运行配置等文件，并且打包而成。\n举个例子，我们要从0部署一个Java应用，大概流程是这样：\n准备一个linux服务（CentOS或者Ubuntu均可） 安装并配置JDK 上传Jar包 运行jar包 那因此，我们打包镜像也是分成这么几步：\n准备Linux运行环境（java项目并不需要完整的操作系统，仅仅是基础运行环境即可） 安装并配置JDK 拷贝jar包 配置启动脚本 上述步骤中的每一次操作其实都是在生产一些文件（系统运行环境、函数库、配置最终都是磁盘文件），所以镜像就是一堆文件的集合。\n但需要注意的是，镜像文件不是随意堆放的，而是按照操作的步骤分层叠加而成，每一层形成的文件都会单独打包并标记一个唯一id，称为Layer（层）。这样，如果我们构建时用到的某些层其他人已经制作过，就可以直接拷贝使用这些层，而不用重复制作。\n例如，第一步中需要的Linux运行环境，通用性就很强，所以Docker官方就制作了这样的只包含Linux运行环境的镜像。我们在制作java镜像时，就无需重复制作，直接使用Docker官方提供的CentOS或Ubuntu镜像作为基础镜像。然后再搭建其它层即可，这样逐层搭建，最终整个Java项目的镜像结构如图所示：\nDockerfile 由于制作镜像的过程中，需要逐层处理和打包，比较复杂，所以Docker就提供了自动打包镜像的功能。我们只需要将打包的过程，每一层要做的事情用固定的语法写下来，交给Docker去执行即可。\n而这种记录镜像结构的文件就称为Dockerfile，其对应的语法可以参考官方文档：\nhttps://docs.docker.com/engine/reference/builder/\n其中的语法比较多，比较常用的有：\n指令 说明 示例 FROM 指定基础镜像 FROM centos:6 ENV 设置环境变量，可在后面指令使用 ENV key value COPY 拷贝本地文件到镜像的指定目录 COPY ./xx.jar /tmp/app.jar RUN 执行Linux的shell命令，一般是安装过程的命令 RUN yum install gcc EXPOSE 指定容器运行时监听的端口，是给镜像使用者看的 EXPOSE 8080 ENTRYPOINT 镜像中应用的启动命令，容器运行时调用 ENTRYPOINT java -jar xx.jar 例如，要基于Ubuntu镜像来构建一个Java应用，其Dockerfile内容如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 # 指定基础镜像 FROM ubuntu:16.04 # 配置环境变量，JDK的安装目录、容器内时区 ENV JAVA_DIR=/usr/local ENV TZ=Asia/Shanghai # 拷贝jdk和java项目的包 COPY ./jdk8.tar.gz $JAVA_DIR/ COPY ./docker-demo.jar /tmp/app.jar # 设定时区 RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone # 安装JDK RUN cd $JAVA_DIR \\ \u0026amp;\u0026amp; tar -xf ./jdk8.tar.gz \\ \u0026amp;\u0026amp; mv ./jdk1.8.0_144 ./java8 # 配置环境变量 ENV JAVA_HOME=$JAVA_DIR/java8 ENV PATH=$PATH:$JAVA_HOME/bin # 指定项目监听的端口 EXPOSE 8080 # 入口，java项目的启动命令 ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;/app.jar\u0026#34;] 同学们思考一下：以后我们会有很多很多java项目需要打包为镜像，他们都需要Linux系统环境、JDK环境这两层，只有上面的3层不同（因为jar包不同）。如果每次制作java镜像都重复制作前两层镜像，是不是很麻烦。\n所以，就有人提供了基础的系统加JDK环境，我们在此基础上制作java镜像，就可以省去JDK的配置了：\n1 2 3 4 5 6 7 8 9 # 基础镜像 FROM openjdk:11.0-jre-buster # 设定时区 ENV TZ=Asia/Shanghai RUN ln -snf /usr/share/zoneinfo/$TZ /etc/localtime \u0026amp;\u0026amp; echo $TZ \u0026gt; /etc/timezone # 拷贝jar包 COPY docker-demo.jar /app.jar # 入口 ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;/app.jar\u0026#34;] 是不是简单多了。\n构建镜像 当Dockerfile文件写好以后，就可以利用命令来构建镜像了。\n在课前资料中，我们准备好了一个demo项目及对应的Dockerfile：\n首先，我们将课前资料提供的docker-demo.jar包以及Dockerfile拷贝到虚拟机的/root/demo目录：\n然后，执行命令，构建镜像：\n1 2 3 4 # 进入镜像目录 cd /root/demo # 开始构建 docker build -t docker-demo:1.0 . 命令说明：\ndocker build : 就是构建一个docker镜像 -t docker-demo:1.0 ：-t参数是指定镜像的名称（repository和tag） . : 最后的点是指构建时Dockerfile所在路径，由于我们进入了demo目录，所以指定的是.代表当前目录，也可以直接指定Dockerfile目录： 结果：\n查看镜像列表：\n1 2 # 直接指定Dockerfile目录 docker build -t docker-demo:1.0 /root/demo 1 2 3 4 5 6 7 # 查看镜像列表： docker images # 结果 REPOSITORY TAG IMAGE ID CREATED SIZE docker-demo 1.0 d6ab0b9e64b9 27 minutes ago 327MB nginx latest 605c77e624dd 16 months ago 141MB mysql latest 3218b38490ce 17 months ago 516MB 然后尝试运行该镜像：\n1 2 3 4 5 6 7 8 9 10 11 12 13 # 1.创建并运行容器 docker run -d --name dd -p 8080:8080 docker-demo:1.0 # 2.查看容器 dps # 结果 CONTAINER ID IMAGE PORTS STATUS NAMES 78a000447b49 docker-demo:1.0 0.0.0.0:8080-\u0026gt;8080/tcp, :::8080-\u0026gt;8080/tcp Up 2 seconds dd f63cfead8502 mysql 0.0.0.0:3306-\u0026gt;3306/tcp, :::3306-\u0026gt;3306/tcp, 33060/tcp Up 2 hours mysql # 3.访问 curl localhost:8080/hello/count # 结果： \u0026lt;h5\u0026gt;欢迎访问黑马商城, 这是您第1次访问\u0026lt;h5\u0026gt; 网络 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 #我们可以使用ip a 命令查看网卡 ip a 展示如下： 1: lo: \u0026lt;LOOPBACK,UP,LOWER_UP\u0026gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: ens33: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:ef:eb:10 brd ff:ff:ff:ff:ff:ff inet 192.168.79.128/24 brd 192.168.79.255 scope global noprefixroute dynamic ens33 valid_lft 1340sec preferred_lft 1340sec inet6 fe80::1f9a:7f21:7dbe:c0d3/64 scope link noprefixroute valid_lft forever preferred_lft forever 3: docker0: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue state UP group default link/ether 02:42:93:59:bb:a6 brd ff:ff:ff:ff:ff:ff inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 valid_lft forever preferred_lft forever inet6 fe80::42:93ff:fe59:bba6/64 scope link valid_lft forever preferred_lft forever 13: veth744375a@if12: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 8a:3c:52:bb:d8:b4 brd ff:ff:ff:ff:ff:ff link-netnsid 1 inet6 fe80::883c:52ff:febb:d8b4/64 scope link valid_lft forever preferred_lft forever 21: veth10c9fcd@if20: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether 86:ad:81:9b:cd:1e brd ff:ff:ff:ff:ff:ff link-netnsid 0 inet6 fe80::84ad:81ff:fe9b:cd1e/64 scope link valid_lft forever preferred_lft forever 25: vethf09cb49@if24: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1500 qdisc noqueue master docker0 state UP group default link/ether d6:84:b5:ad:c3:85 brd ff:ff:ff:ff:ff:ff link-netnsid 2 inet6 fe80::d484:b5ff:fead:c385/64 scope link valid_lft forever preferred_lft forever 上节课我们创建了一个Java项目的容器，而Java项目往往需要访问其它各种中间件，例如MySQL、Redis等。现在，我们的容器之间能否互相访问呢？我们来测试一下\n首先，我们查看下MySQL容器的详细信息，重点关注其中的网络IP地址：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 # 1.用基本命令，寻找Networks.bridge.IPAddress属性 docker inspect mysql # 也可以使用format过滤结果 docker inspect --format=\u0026#39;{{range .NetworkSettings.Networks}}{{println .IPAddress}}{{end}}\u0026#39; mysql # 得到IP地址如下： 172.17.0.2 # 2.然后通过命令进入dd容器 docker exec -it dd bash # 3.在容器内，通过ping命令测试网络 ping 172.17.0.2 # 结果 PING 172.17.0.2 (172.17.0.2) 56(84) bytes of data. 64 bytes from 172.17.0.2: icmp_seq=1 ttl=64 time=0.053 ms 64 bytes from 172.17.0.2: icmp_seq=2 ttl=64 time=0.059 ms 64 bytes from 172.17.0.2: icmp_seq=3 ttl=64 time=0.058 ms 发现可以互联，没有问题。\n但是，容器的网络IP其实是一个虚拟的IP，其值并不固定与某一个容器绑定，如果我们在开发时写死某个IP，而在部署时很可能MySQL容器的IP会发生变化，连接会失败。\n所以，我们必须借助于docker的网络功能来解决这个问题，官方文档：\nhttps://docs.docker.com/engine/reference/commandline/network/\n加入自定义网络的容器才可以通过容器名互相访问，默认网络没有此功能\n常见命令有：\n命令 说明 文档地址 docker network create 创建一个网络 docker network create docker network ls 查看所有网络 docs.docker.com docker network rm 删除指定网络 docs.docker.com docker network prune 清除未使用的网络 docs.docker.com docker network connect 使指定容器连接加入某网络 docs.docker.com docker network disconnect 使指定容器连接离开某网络 docker network disconnect docker network inspect 查看网络详细信息 docker network inspect 教学演示：自定义网络\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 # 1.首先通过命令创建一个网络 docker network create hmall # 2.然后查看网络 docker network ls # 结果： NETWORK ID NAME DRIVER SCOPE 639bc44d0a87 bridge bridge local 403f16ec62a2 hmall bridge local 0dc0f72a0fbb host host local cd8d3e8df47b none null local # 其中，除了hmall以外，其它都是默认的网络 # 3.让dd和mysql都加入该网络，注意，在加入网络时可以通过--alias给容器起别名 # 这样该网络内的其它容器可以用别名互相访问！ # 3.1.mysql容器，指定别名为db，另外每一个容器都有一个别名是容器名 docker network connect hmall mysql --alias db # 3.2.dd容器，也就是我们的java项目 docker network connect hmall dd # 4.进入dd容器，尝试利用别名访问db # 4.1.进入容器 docker exec -it dd bash # 4.2.用db别名访问 ping db # 结果 PING db (172.18.0.2) 56(84) bytes of data. 64 bytes from mysql.hmall (172.18.0.2): icmp_seq=1 ttl=64 time=0.070 ms 64 bytes from mysql.hmall (172.18.0.2): icmp_seq=2 ttl=64 time=0.056 ms # 4.3.用容器名访问 ping mysql # 结果： PING mysql (172.18.0.2) 56(84) bytes of data. 64 bytes from mysql.hmall (172.18.0.2): icmp_seq=1 ttl=64 time=0.044 ms 64 bytes from mysql.hmall (172.18.0.2): icmp_seq=2 ttl=64 time=0.054 ms # 也可以在创建容器时指定容器的网络,这样容器只会加入此网络，不会加入默认网络 docker run -d --name dd -p 8080:8080 --network heima docker-demo OK，现在无需记住IP地址也可以实现容器互联了。\n总结：\n在自定义网络中，可以给容器起多个别名，默认的别名是容器名本身 在同一个自定义网络中的容器，可以通过别名互相访问 ","date":"2024-01-18T21:50:56+08:00","image":"https://xxl999227.github.io/archives/docker%E7%AF%873docker%E5%9F%BA%E7%A1%80/img/image.png","permalink":"https://xxl999227.github.io/archives/docker%E7%AF%873docker%E5%9F%BA%E7%A1%80/","title":"【docker篇】3、docker基础"},{"content":"部署MySQL 首先，我们利用Docker来安装一个MySQL软件，大家可以对比一下之前传统的安装方式，看看哪个效率更高一些。\n如果是利用传统方式部署MySQL，大概的步骤有：\n搜索并下载MySQL安装包 上传至Linux环境 编译和配置环境 安装 而使用Docker安装，仅仅需要一步即可，在命令行输入下面的命令（建议采用CV大法）：\n1 2 3 4 5 6 docker run -d \\ --name mysql \\ -p 3306:3306 \\ -e TZ=Asia/Shanghai \\ -e MYSQL_ROOT_PASSWORD=123456 \\ mysql 运行效果如图：\nMySQL安装完毕！通过任意客户端工具即可连接到MySQL.\n大家可以发现，当我们执行命令后，Docker做的第一件事情，是去自动搜索并下载了MySQL，然后会自动运行MySQL，我们完全不用插手，是不是非常方便。\n而且，这种安装方式你完全不用考虑运行的操作系统环境，它不仅仅在CentOS系统是这样，在Ubuntu系统、macOS系统、甚至是装了WSL的Windows下，都可以使用这条命令来安装MySQL。\n要知道，不同操作系统下其安装包、运行环境是都不相同的！如果是手动安装，必须手动解决安装包不同、环境不同的、配置不同的问题！\n而使用Docker，这些完全不用考虑。就是因为Docker会自动搜索并下载MySQL。注意：这里下载的不是安装包，而是镜像。镜像中不仅包含了MySQL本身，还包含了其运行所需要的环境、配置、系统级函数库。因此它在运行时就有自己独立的环境，就可以跨系统运行，也不需要手动再次配置环境了。这套独立运行的隔离环境我们称为容器。\n说明：\n镜像：英文是image 容器：英文是container 因此，Docker安装软件的过程，就是自动搜索下载镜像，然后创建并运行容器的过程。\nDocker会根据命令中的镜像名称自动搜索并下载镜像，那么问题来了，它是去哪里搜索和下载镜像的呢？这些镜像又是谁制作的呢？\nDocker官方提供了一个专门管理、存储镜像的网站，并对外开放了镜像上传、下载的权利。Docker官方提供了一些基础镜像，然后各大软件公司又在基础镜像基础上，制作了自家软件的镜像，全部都存放在这个网站。这个网站就成了Docker镜像交流的社区：\nhttps://hub.docker.com/\n基本上我们常用的各种软件都能在这个网站上找到，我们甚至可以自己制作镜像上传上去。\n像这种提供存储、管理Docker镜像的服务器，被称为DockerRegistry，可以翻译为镜像仓库。DockerHub网站是官方仓库，阿里云、华为云会提供一些第三方仓库，我们也可以自己搭建私有的镜像仓库。\n官方仓库在国外，下载速度较慢，一般我们都会使用第三方仓库提供的镜像加速功能，提高下载速度。而企业内部的机密项目，往往会采用私有镜像仓库。\n总之，镜像的来源有两种：\n基于官方基础镜像自己制作 直接去DockerRegistry下载 总结一下：\nDocker本身包含一个后台服务，我们可以利用Docker命令告诉Docker服务，帮助我们快速部署指定的应用。Docker服务部署应用时，首先要去搜索并下载应用对应的镜像，然后根据镜像创建并允许容器，应用就部署完成了。\n用一幅图表示如下：\n命令解读 利用Docker快速的安装了MySQL，非常的方便，不过我们执行的命令到底是什么意思呢？\n其实是一行命令，用 \\ 做了换行\n1 2 3 4 5 6 docker run -d \\ --name mysql \\ -p 3306:3306 \\ -e TZ=Asia/Shanghai \\ -e MYSQL_ROOT_PASSWORD=123 \\ mysql 解读：\ndocker run -d ：创建并运行一个容器，-d则是让容器以后台进程运行，基本上都是搭配-d使用 \u0026ndash;name mysql : 给容器起个名字叫mysql，可以叫别的 -p 3306:3306 : 设置端口映射。 容器是隔离环境，外界不可访问。但是可以将宿主机端口****映射容器内到端口，当访问宿主机指定端口时，就是在访问容器内的端口了。 容器内端口往往是由容器内的进程决定，例如MySQL进程默认端口是3306，因此容器内端口一定是3306；而宿主机端口则可以任意指定，一般与容器内保持一致。 格式： -p 宿主机端口:容器内端口，示例中就是将宿主机的3306映射到容器内的3306端口 -e TZ=Asia/Shanghai : 配置容器内进程运行时的一些参数 格式：-e KEY=VALUE，KEY和VALUE都由容器内进程决定 案例中，TZ=Asia/Shanghai是设置时区；MYSQL_ROOT_PASSWORD=123是设置MySQL默认密码 mysql : 设置镜像名称，Docker会根据这个名字搜索并下载镜像 格式：REPOSITORY:TAG，例如mysql:8.0，其中REPOSITORY可以理解为镜像名，TAG是版本号 在未指定TAG的情况下，默认是最新版本，也就是mysql:latest 镜像的名称不是随意的，而是要到DockerRegistry中寻找，镜像运行时的配置也不是随意的，要参考镜像的帮助文档，这些在DockerHub网站或者软件的官方网站中都能找到。\n如果我们要安装其它软件，也可以到DockerRegistry中寻找对应的镜像名称和版本，阅读相关配置即可。\n","date":"2024-01-17T21:33:14+08:00","image":"https://xxl999227.github.io/archives/docker%E7%AF%872%E5%AE%89%E8%A3%85mysql/img/image.png","permalink":"https://xxl999227.github.io/archives/docker%E7%AF%872%E5%AE%89%E8%A3%85mysql/","title":"【docker篇】2、安装Mysql"},{"content":"安装Docker\n本安装教程参考Docker官方文档，地址如下：\nhttps://docs.docker.com/engine/install/centos/\n卸载旧版 首先如果系统中已经存在旧的Docker，则先卸载：\n1 2 3 4 5 6 7 8 yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine 配置Docker的yum库 首先要安装一个yum工具\n1 yum install -y yum-utils 安装成功后，执行命令，配置Docker的yum源：\n1 yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 这里建议阿里云的yum源\n1 yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装Docker 最后，执行命令，安装Docker\n1 yum install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin 启动和校验 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # 启动Docker systemctl start docker # 停止Docker systemctl stop docker # 重启 systemctl restart docker # 设置开机自启 systemctl enable docker # 执行docker ps命令，如果不报错，说明安装启动成功 docker ps 配置镜像加速 这里以阿里云镜像加速为例。\n注册阿里云账号 首先访问阿里云网站:\nhttps://www.aliyun.com/\n注册一个账号。\n开通镜像服务 在首页的产品中，找到阿里云的容器镜像服务：\n点击后进入控制台：\n首次可能需要选择立刻开通，然后进入控制台。\n配置镜像加速 找到镜像工具下的镜像****加速器：\n页面向下滚动，即可找到配置的文档说明：\n具体命令如下：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # 创建目录 mkdir -p /etc/docker # 复制内容，注意把其中的镜像加速地址改成你自己的 tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://xxxx.mirror.aliyuncs.com\u0026#34;] } EOF # 重新加载配置 systemctl daemon-reload # 重启Docker systemctl restart docker ","date":"2023-12-28T23:10:19+08:00","image":"https://xxl999227.github.io/archives/docker%E7%AF%871%E5%AE%89%E8%A3%85docker/img/image.png","permalink":"https://xxl999227.github.io/archives/docker%E7%AF%871%E5%AE%89%E8%A3%85docker/","title":"【docker篇】1、安装Docker"}]